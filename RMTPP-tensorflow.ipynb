{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.keras import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pad_sequences = preprocessing.sequence.pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "from scipy.integrate import quad, quad_explain\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'rmtpp-data/real/so/'\n",
    "idx = 1\n",
    "event_train_file = os.path.join(path, f'event-{idx}-train.txt')\n",
    "event_test_file = os.path.join(path, f'event-{idx}-test.txt')\n",
    "time_train_file = os.path.join(path, f'time-{idx}-train.txt')\n",
    "time_test_file = os.path.join(path, f'time-{idx}-test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(event_train_file, 'r') as in_file:\n",
    "    eventTrain = [[int(y) for y in x.strip().split()] for x in in_file]\n",
    "\n",
    "with open(event_test_file, 'r') as in_file:\n",
    "    eventTest = [[int(y) for y in x.strip().split()] for x in in_file]\n",
    "\n",
    "with open(time_train_file, 'r') as in_file:\n",
    "    timeTrain = [[float(y) for y in x.strip().split()] for x in in_file]\n",
    "\n",
    "with open(time_test_file, 'r') as in_file:\n",
    "    timeTest = [[float(y) for y in x.strip().split()] for x in in_file]\n",
    "\n",
    "assert len(timeTrain) == len(eventTrain)\n",
    "assert len(eventTest) == len(timeTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_samples = len(eventTrain)\n",
    "max_seqlen = max(len(x) for x in eventTrain)\n",
    "unique_samples = set()\n",
    "for x in eventTrain + eventTest:\n",
    "    unique_samples = unique_samples.union(x)\n",
    "    \n",
    "maxTime = max(itertools.chain((max(x) for x in timeTrain), (max(x) for x in timeTest)))\n",
    "minTime = min(itertools.chain((min(x) for x in timeTrain), (min(x) for x in timeTest)))\n",
    "# minTime, maxTime = 0, 1\n",
    "\n",
    "eventTrainIn = [x[:-1] for x in eventTrain]\n",
    "eventTrainOut = [x[1:] for x in eventTrain]\n",
    "timeTrainIn = [[(y - minTime) / (maxTime - minTime) for y in x[:-1]] for x in timeTrain]\n",
    "timeTrainOut = [[(y - minTime) / (maxTime - minTime) for y in x[1:]] for x in timeTrain]\n",
    "\n",
    "train_event_in_seq = pad_sequences(eventTrainIn, padding='post')\n",
    "train_event_out_seq = pad_sequences(eventTrainOut, padding='post')\n",
    "train_time_in_seq = pad_sequences(timeTrainIn, dtype=float, padding='post')\n",
    "train_time_out_seq = pad_sequences(timeTrainOut, dtype=float, padding='post')\n",
    "\n",
    "eventTestIn = [x[:-1] for x in eventTest]\n",
    "eventTestOut = [x[1:] for x in eventTest]\n",
    "timeTestIn = [[(y - minTime) / (maxTime - minTime) for y in x[:-1]] for x in timeTest]\n",
    "timeTestOut = [[(y - minTime) / (maxTime - minTime) for y in x[1:]] for x in timeTest]\n",
    "\n",
    "test_event_in_seq = pad_sequences(eventTestIn, padding='post')\n",
    "test_event_out_seq = pad_sequences(eventTestOut, padding='post')\n",
    "test_time_in_seq = pad_sequences(timeTestIn, dtype=float, padding='post')\n",
    "test_time_out_seq = pad_sequences(timeTestOut, dtype=float, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not doing one-hot encoding because TF provides tf.nn.embedding_lookup\n",
    "\n",
    "# nb_events = len(unique_samples)\n",
    "\n",
    "# train_event_out_hot_seq = np.zeros((nb_samples, max_seqlen - 1, nb_events), dtype=int)\n",
    "\n",
    "# for ii, evs in enumerate(eventTrainOut):\n",
    "#     for jj, x in enumerate(evs):\n",
    "#         train_event_out_hot_seq[ii, jj, x - 1] = 1a\n",
    "        \n",
    "# nb_tests = len(eventTest)\n",
    "\n",
    "# max_test_seqlen = max(len(x) for x in eventTest)\n",
    "# test_event_out_hot_seq = np.zeros((nb_tests, max_test_seqlen - 1, nb_events), dtype=int)\n",
    "\n",
    "# for ii, evs in enumerate(eventTestOut):\n",
    "#     for jj, x in enumerate(evs):\n",
    "#         test_event_out_hot_seq[ii, jj, x - 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assert np.sum(test_event_out_hot_seq) == sum(len(x) for x in eventTestOut)\n",
    "# assert np.sum(train_event_out_hot_seq) == sum(len(x) for x in eventTrainOut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIDDEN_LAYER_SIZE = 128 # 64, 128, 256, 512, 1024\n",
    "BATCH_SIZE = 28 # 16, 32, 64\n",
    "LEARNING_RATE = 0.1 # 0.1, 0.01, 0.001\n",
    "MOMENTUM = 0.9\n",
    "L2_PENALTY = 0.001\n",
    "EMBED_SIZE = 100 # ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CATEGORIES = len(unique_samples)\n",
    "FLOAT_TYPE = tf.float32\n",
    "RNN_CELL_TYPE = tf.contrib.rnn.GRUCell\n",
    "BPTT = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "seed = 42\n",
    "RS = np.random.RandomState(seed)\n",
    "scope = \"RMTPP\"\n",
    "\n",
    "with tf.variable_scope(scope):\n",
    "\n",
    "    with tf.device('/gpu:0'):\n",
    "        # Make input variables\n",
    "        events_in = tf.placeholder(tf.int32, [BATCH_SIZE, BPTT])\n",
    "        times_in = tf.placeholder(FLOAT_TYPE, [BATCH_SIZE, BPTT])\n",
    "\n",
    "        events_out = tf.placeholder(tf.int32, [BATCH_SIZE, BPTT])\n",
    "        times_out = tf.placeholder(FLOAT_TYPE, [BATCH_SIZE, BPTT])\n",
    "\n",
    "        # Make variables\n",
    "        with tf.variable_scope('hidden_state'):\n",
    "            Wt = tf.get_variable(name='Wt', shape=(1, HIDDEN_LAYER_SIZE), \n",
    "                                 dtype=FLOAT_TYPE)            \n",
    "            # The first row of Wem is merely a placeholder (will not be trained).\n",
    "            Wem = tf.get_variable(name='Wem', shape=(NUM_CATEGORIES + 1, EMBED_SIZE), \n",
    "                                  dtype=FLOAT_TYPE)\n",
    "            Wh = tf.get_variable(name='Wh', shape=(HIDDEN_LAYER_SIZE, HIDDEN_LAYER_SIZE), \n",
    "                                 dtype=FLOAT_TYPE)\n",
    "            bh = tf.get_variable(name='bh', shape=(1, HIDDEN_LAYER_SIZE),\n",
    "                                 dtype=FLOAT_TYPE)\n",
    "            \n",
    "        with tf.variable_scope('output'):\n",
    "            wt = tf.get_variable(name='wt', shape=(1, 1), \n",
    "                                 dtype=FLOAT_TYPE)\n",
    "\n",
    "            Wy = tf.get_variable(name='Wy', shape=(EMBED_SIZE, HIDDEN_LAYER_SIZE), \n",
    "                             dtype=FLOAT_TYPE)\n",
    "\n",
    "            # The first column of Vy is merely a placeholder (will not be trained).\n",
    "            Vy = tf.get_variable(name='Vy', shape=(HIDDEN_LAYER_SIZE, NUM_CATEGORIES + 1),\n",
    "                                 dtype=FLOAT_TYPE)\n",
    "            Vt = tf.get_variable(name='Vt', shape=(HIDDEN_LAYER_SIZE, 1),\n",
    "                                 dtype=FLOAT_TYPE,\n",
    "                                 initializer=tf.uniform_unit_scaling_initializer())\n",
    "            bt = tf.get_variable(name='bt', shape=(1, 1),\n",
    "                                 dtype=FLOAT_TYPE)\n",
    "            bk = tf.get_variable(name='bk', shape=(1, NUM_CATEGORIES + 1),\n",
    "                                 dtype=FLOAT_TYPE)\n",
    "\n",
    "        # Make graph    \n",
    "        # RNNcell = RNN_CELL_TYPE(HIDDEN_LAYER_SIZE)\n",
    "\n",
    "        # Initial state for GRU cells\n",
    "        initial_state = state = tf.zeros([BATCH_SIZE, HIDDEN_LAYER_SIZE], dtype=FLOAT_TYPE, name='hidden_state')\n",
    "\n",
    "        loss = 0.0\n",
    "        batch_ones = tf.ones((BATCH_SIZE, 1), dtype=FLOAT_TYPE)\n",
    "        for i in range(BPTT):\n",
    "            events_embedded = tf.nn.embedding_lookup(Wem, events_in[:, i])\n",
    "            time = tf.expand_dims(times_in[:, i], axis=-1)\n",
    "\n",
    "            # output, state = RNNcell(events_embedded, state)\n",
    "            # TODO Does TF automatically broadcast? Then we'll not need multiplication\n",
    "            # with tf.ones\n",
    "\n",
    "            state = tf.clip_by_value(\n",
    "                tf.matmul(state, Wh) + \n",
    "                tf.matmul(events_embedded, Wy) + \n",
    "                tf.matmul(time, Wt) + \n",
    "                tf.matmul(batch_ones, bh), \n",
    "                0.0, 1e6, \n",
    "                name='h_t')\n",
    "\n",
    "            base_intensity = tf.matmul(batch_ones, bt)\n",
    "            delta_t = tf.expand_dims(times_out[:, i] - times_in[:, i], axis=-1)\n",
    "            log_lambda_ = (tf.matmul(state, Vt) + \n",
    "                           delta_t * wt + \n",
    "                           base_intensity)\n",
    "\n",
    "            lambda_ = tf.exp(tf.minimum(50.0, log_lambda_), name='lambda_')\n",
    "            wt_non_zero = tf.sign(wt) * tf.maximum(1e-6, tf.abs(wt))\n",
    "            log_f_star = (log_lambda_ + \n",
    "                          (1/wt_non_zero) * tf.exp(tf.minimum(50.0, tf.matmul(state, Vt) + base_intensity)) -\n",
    "                          (1/wt_non_zero) * lambda_)\n",
    "\n",
    "            events_pred = tf.nn.softmax(tf.minimum(50.0, \n",
    "                                                   tf.matmul(state, Vy) + batch_ones * bk),\n",
    "                                        name='Pr_events'\n",
    "                                       )\n",
    "\n",
    "            time_loss = log_f_star\n",
    "            mark_loss = tf.expand_dims(\n",
    "                tf.log(\n",
    "                    tf.maximum(1e-6, \n",
    "                        tf.gather_nd(\n",
    "                            events_pred, \n",
    "                            tf.concat([\n",
    "                                tf.expand_dims(tf.range(BATCH_SIZE), -1),\n",
    "                                tf.expand_dims(events_out[:, i], -1)                             \n",
    "                            ], axis=1, name='Pr_next_event'\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                ), axis=-1, name='log_Pr_next_event'\n",
    "            )\n",
    "            step_loss = time_loss + mark_loss\n",
    "\n",
    "            # In the batch some of the sequences may have ended before we get to the\n",
    "            # end of the seq. In such cases, the events will be zero.\n",
    "            # TODO Figure out how to do this with RNNCell, LSTM, etc.\n",
    "            num_events = tf.reduce_sum(tf.where(events_in[:, i] > 0, \n",
    "                                       tf.ones(BATCH_SIZE, dtype=FLOAT_TYPE), \n",
    "                                       tf.zeros(BATCH_SIZE, dtype=FLOAT_TYPE)),\n",
    "                                       name='num_events')\n",
    "            loss -= tf.cond(num_events > 0, \n",
    "                            lambda: tf.reduce_sum(tf.where(events_in[:, i] > 0, \n",
    "                                               tf.squeeze(step_loss) / num_events,\n",
    "                                               tf.zeros(BATCH_SIZE)), name='batch_bptt_loss'),\n",
    "                            lambda: 0.0)\n",
    "\n",
    "        final_state = state\n",
    "        # optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        # update = optimizer.minimize(loss)\n",
    "\n",
    "        # Performing manual gradient clipping.\n",
    "        gvs = optimizer.compute_gradients(loss)\n",
    "        # update = optimizer.apply_gradients(gvs)\n",
    "\n",
    "        # capped_gvs = [(tf.clip_by_norm(grad, 100.0), var) for grad, var in gvs]\n",
    "        grads, vars_ = list(zip(*gvs))\n",
    "        norm_grads, global_norm = tf.clip_by_global_norm(grads, 100.0)\n",
    "        capped_gvs = list(zip(norm_grads, vars_))\n",
    "        \n",
    "        update = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        check_nan = tf.add_check_numerics_ops()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Creation of batches and execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterSession.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterSession = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.get_variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.assign?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch... 0\n",
      "Loss on last batch = 2.000617855501481e+18\n",
      "Loss on last batch = 2.0327304970371536e+16\n",
      "Loss on last batch = -477.50348234176636\n",
      "Loss on last batch = -524.0962543487549\n",
      "Loss on last batch = -281.86440992355347\n",
      "Loss on last batch = -747.6655303239822\n",
      "Loss on last batch = -446.08779191970825\n",
      "Loss on last batch = -1390.4635381698608\n",
      "Loss on last batch = -618.6797442436218\n",
      "Loss on last batch = -435.52120941877365\n",
      "Loss on last batch = -294.66334676742554\n",
      "Loss on last batch = -160.44714260101318\n",
      "Loss on last batch = -469.56241607666016\n",
      "Loss on last batch = -249.19526720046997\n",
      "Loss on last batch = -456.31859052181244\n",
      "Loss on last batch = -240.49977684020996\n",
      "Loss on last batch = -282.42193365097046\n",
      "Loss on last batch = -712.6429274082184\n",
      "Loss on last batch = -336.95236110687256\n",
      "Starting epoch... 1\n",
      "Loss on last batch = -331.9180746078491\n",
      "Loss on last batch = -816.9900501966476\n",
      "Loss on last batch = -245.63119220733643\n",
      "Loss on last batch = -190.26225662231445\n",
      "Loss on last batch = -435.84371757507324\n",
      "Loss on last batch = -290.6644639968872\n",
      "Loss on last batch = -314.3267254829407\n",
      "Loss on last batch = -714.1663513183594\n",
      "Loss on last batch = -1029.5534942150116\n",
      "Loss on last batch = -342.81489849090576\n",
      "Loss on last batch = -373.69641494750977\n",
      "Loss on last batch = -105.37614345550537\n",
      "Loss on last batch = -125.90613460540771\n",
      "Loss on last batch = -615.7373371124268\n",
      "Loss on last batch = -684.2173771858215\n",
      "Loss on last batch = -314.2054295539856\n",
      "Loss on last batch = -314.7913236618042\n",
      "Loss on last batch = -336.2216782569885\n",
      "Loss on last batch = -245.9078831076622\n",
      "Starting epoch... 2\n",
      "Loss on last batch = -371.2157940864563\n",
      "Loss on last batch = -242.29105758666992\n",
      "Loss on last batch = -971.9762935638428\n",
      "Loss on last batch = -824.216588973999\n",
      "Loss on last batch = -423.4293031692505\n",
      "Loss on last batch = -246.88073921203613\n",
      "Loss on last batch = -431.2421090602875\n",
      "Loss on last batch = -289.9283962249756\n",
      "Loss on last batch = -912.7662038803101\n",
      "Loss on last batch = -290.59797191619873\n",
      "Loss on last batch = -223.99974536895752\n",
      "Loss on last batch = -222.03324699401855\n",
      "Loss on last batch = -771.8805847167969\n",
      "Loss on last batch = -339.6303987503052\n",
      "Loss on last batch = -128.04945516586304\n",
      "Loss on last batch = -576.5900735855103\n",
      "Loss on last batch = -182.56664657592773\n",
      "Loss on last batch = -749.0766878128052\n",
      "Loss on last batch = -608.8850440979004\n",
      "Starting epoch... 3\n",
      "Loss on last batch = -222.18988037109375\n",
      "Loss on last batch = -333.6301574707031\n",
      "Loss on last batch = -625.5960960388184\n",
      "Loss on last batch = -615.564866065979\n",
      "Loss on last batch = -808.0977973937988\n",
      "Loss on last batch = -416.5205900669098\n",
      "Loss on last batch = -386.7157039642334\n",
      "Loss on last batch = -293.112690448761\n",
      "Loss on last batch = -444.33342576026917\n",
      "Loss on last batch = -178.169997215271\n",
      "Loss on last batch = -720.5853729248047\n",
      "Loss on last batch = -1044.986397743225\n",
      "Loss on last batch = -167.0842571258545\n",
      "Loss on last batch = -589.9902811050415\n",
      "Loss on last batch = -752.1510019302368\n",
      "Loss on last batch = -394.9931640625\n",
      "Loss on last batch = -195.27815914154053\n",
      "Loss on last batch = -198.33759367465973\n",
      "Loss on last batch = -252.95679378509521\n",
      "Starting epoch... 4\n",
      "Loss on last batch = -311.51451683044434\n",
      "Loss on last batch = -203.9840955734253\n",
      "Loss on last batch = -255.45581150054932\n",
      "Loss on last batch = -398.1334329843521\n",
      "Loss on last batch = -910.2371158599854\n",
      "Loss on last batch = -530.5586733818054\n",
      "Loss on last batch = -406.5355587005615\n",
      "Loss on last batch = -658.692156791687\n",
      "Loss on last batch = -643.4619283676147\n",
      "Loss on last batch = -493.3762774467468\n",
      "Loss on last batch = -527.5189990997314\n",
      "Loss on last batch = -151.75952291488647\n",
      "Loss on last batch = -390.7952480316162\n",
      "Loss on last batch = -285.38793087005615\n",
      "Loss on last batch = -306.08296859264374\n",
      "Loss on last batch = -181.39328575134277\n",
      "Loss on last batch = -474.08727645874023\n",
      "Loss on last batch = -714.8626463413239\n",
      "Loss on last batch = -461.1678657531738\n",
      "CPU times: user 40min 17s, sys: 15min 24s, total: 55min 42s\n",
      "Wall time: 28min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "idxes = list(range(len(eventTrainIn)))\n",
    "\n",
    "rs = np.random.RandomState(seed=42)\n",
    "\n",
    "iterSession.run(init)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    rs.shuffle(idxes)\n",
    "    \n",
    "    print(\"Starting epoch...\", epoch)\n",
    "    \n",
    "    for batch_idx in range(len(idxes) // BATCH_SIZE):\n",
    "        batch_idxes = idxes[batch_idx * BATCH_SIZE:(batch_idx + 1) * BATCH_SIZE]\n",
    "        batch_event_train_in = train_event_in_seq[batch_idxes, :]\n",
    "        batch_event_train_out = train_event_out_seq[batch_idxes, :]\n",
    "        batch_time_train_in = train_time_in_seq[batch_idxes, :]\n",
    "        batch_time_train_out = train_time_out_seq[batch_idxes, :]\n",
    "        \n",
    "        cur_state = np.zeros((BATCH_SIZE, HIDDEN_LAYER_SIZE))\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for bptt_idx in range(0, len(batch_event_train_in[0]) - BPTT, BPTT):\n",
    "            bptt_range = range(bptt_idx, (bptt_idx + BPTT))\n",
    "            bptt_event_in = batch_event_train_in[:, bptt_range]\n",
    "            bptt_event_out = batch_event_train_out[:, bptt_range]\n",
    "            bptt_time_in = batch_time_train_in[:, bptt_range]\n",
    "            bptt_time_out = batch_time_train_out[:, bptt_range]\n",
    "            \n",
    "            feed_dict = {\n",
    "                  initial_state: cur_state,\n",
    "                  events_in: bptt_event_in,\n",
    "                  events_out: bptt_event_out,\n",
    "                  times_in: bptt_time_in,\n",
    "                  times_out: bptt_time_out\n",
    "            }\n",
    "            \n",
    "#             _, _, cur_state, loss_ = \\\n",
    "#                 iterSession.run([check_nan, update, final_state, loss],\n",
    "#                                feed_dict=feed_dict)\n",
    "            _, cur_state, loss_ = \\\n",
    "                iterSession.run([update, final_state, loss],\n",
    "                                feed_dict=feed_dict)\n",
    "            total_loss += loss_\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Loss on last batch = {}'.format(total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = tf.get_default_graph().get_tensor_by_name('truediv_11:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterSession.run(num_events, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('logs', iterSession.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.48264164],\n",
       "       [ 0.51298624],\n",
       "       [-0.48903847],\n",
       "       [ 0.06690322],\n",
       "       [ 0.43058664],\n",
       "       [ 0.42051542],\n",
       "       [ 0.14833394],\n",
       "       [-0.40770617],\n",
       "       [ 0.54140252],\n",
       "       [ 0.50183517],\n",
       "       [-0.21103132],\n",
       "       [ 0.61192852],\n",
       "       [ 0.56355983],\n",
       "       [ 0.64011478],\n",
       "       [ 0.23290093],\n",
       "       [ 0.10714544],\n",
       "       [-0.12148382],\n",
       "       [ 0.32047951],\n",
       "       [-0.10559842],\n",
       "       [-0.20540164],\n",
       "       [ 0.55692488],\n",
       "       [-0.08733884],\n",
       "       [ 0.11255063],\n",
       "       [ 0.49642518],\n",
       "       [ 0.03260658],\n",
       "       [ 0.1046463 ],\n",
       "       [ 0.53848338],\n",
       "       [ 0.04741428],\n",
       "       [-0.03976917],\n",
       "       [ 0.38728651],\n",
       "       [ 0.03960391],\n",
       "       [ 0.03584621],\n",
       "       [ 0.55425429],\n",
       "       [ 0.45849377],\n",
       "       [ 0.43218639],\n",
       "       [ 0.08550103],\n",
       "       [ 0.22151947],\n",
       "       [ 0.11608371],\n",
       "       [ 0.07047138],\n",
       "       [ 0.40548283],\n",
       "       [-0.05357005],\n",
       "       [ 0.04481015],\n",
       "       [-0.09771542],\n",
       "       [ 0.16430897],\n",
       "       [-0.01030764],\n",
       "       [-0.0459405 ],\n",
       "       [ 0.22375916],\n",
       "       [-0.28097811],\n",
       "       [ 0.50770235],\n",
       "       [ 0.49260226],\n",
       "       [ 0.51858354],\n",
       "       [ 0.04118187],\n",
       "       [-0.08957747],\n",
       "       [ 0.03196302],\n",
       "       [ 0.59487724],\n",
       "       [ 0.20243265],\n",
       "       [ 0.07255176],\n",
       "       [ 0.44257143],\n",
       "       [ 0.24585484],\n",
       "       [ 0.55692142],\n",
       "       [ 0.00428808],\n",
       "       [ 0.52216297],\n",
       "       [-0.1132708 ],\n",
       "       [ 0.42256376],\n",
       "       [-0.04508691],\n",
       "       [ 0.531407  ],\n",
       "       [ 0.41995418],\n",
       "       [ 0.4058142 ],\n",
       "       [ 0.5083276 ],\n",
       "       [ 0.42588857],\n",
       "       [-0.01728831],\n",
       "       [ 0.58894563],\n",
       "       [-0.05670552],\n",
       "       [ 0.65347308],\n",
       "       [ 0.5991928 ],\n",
       "       [ 0.0071101 ],\n",
       "       [ 0.29051599],\n",
       "       [ 0.29590437],\n",
       "       [ 0.29211706],\n",
       "       [ 0.45687681],\n",
       "       [ 0.5818615 ],\n",
       "       [ 0.62102622],\n",
       "       [ 0.37551591],\n",
       "       [-0.07528957],\n",
       "       [ 0.48462409],\n",
       "       [ 0.51276243],\n",
       "       [ 0.56302834],\n",
       "       [ 0.0506549 ],\n",
       "       [ 0.05587143],\n",
       "       [ 0.47659522],\n",
       "       [-0.12069833],\n",
       "       [ 0.54263771],\n",
       "       [ 0.56426042],\n",
       "       [ 0.44548264],\n",
       "       [ 0.08146219],\n",
       "       [ 0.02684553],\n",
       "       [ 0.44540998],\n",
       "       [ 0.24162696],\n",
       "       [ 0.55796009],\n",
       "       [ 0.27353019],\n",
       "       [ 0.20673726],\n",
       "       [ 0.60970837],\n",
       "       [ 0.61890203],\n",
       "       [ 0.62993073],\n",
       "       [-0.08751854],\n",
       "       [-0.0243611 ],\n",
       "       [ 0.55680251],\n",
       "       [ 0.3060309 ],\n",
       "       [ 0.39186752],\n",
       "       [ 0.16037822],\n",
       "       [ 0.48193952],\n",
       "       [ 0.4686079 ],\n",
       "       [-0.10136667],\n",
       "       [ 0.04793224],\n",
       "       [-0.03116905],\n",
       "       [ 0.57269883],\n",
       "       [ 0.57370698],\n",
       "       [ 0.63608402],\n",
       "       [ 0.27106109],\n",
       "       [ 0.26397935],\n",
       "       [ 0.38308415],\n",
       "       [ 0.09690355],\n",
       "       [ 0.0590197 ],\n",
       "       [ 0.13266249],\n",
       "       [ 0.14287016],\n",
       "       [ 0.45064709],\n",
       "       [ 0.02174828],\n",
       "       [ 0.60951024]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1000000.,  1000000.,        0., ...,  1000000.,        0.,\n",
       "         1000000.],\n",
       "       [ 1000000.,  1000000.,        0., ...,  1000000.,        0.,\n",
       "         1000000.],\n",
       "       [ 1000000.,  1000000.,        0., ...,  1000000.,        0.,\n",
       "         1000000.],\n",
       "       ..., \n",
       "       [ 1000000.,  1000000.,        0., ...,  1000000.,        0.,\n",
       "         1000000.],\n",
       "       [ 1000000.,  1000000.,        0., ...,  1000000.,        0.,\n",
       "         1000000.],\n",
       "       [ 1000000.,  1000000.,        0., ...,  1000000.,        0.,\n",
       "         1000000.]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state.eval(feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting next-event-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fp(w, delta):\n",
    "    \"\"\"Solve the equation x * exp(-w * x) = delta uding Fixed point iteration, finding the \n",
    "    solution beyond the maximum.\"\"\"\n",
    "    # t = - 10 * np.log(w) * (1 / w)\n",
    "    t = 2 * (1 / w)\n",
    "    i = 0\n",
    "    while True:\n",
    "        # This iteration finds the wrong solution (the one close to origin)\n",
    "        # t_next = np.exp(w * t) * delta \n",
    "        \n",
    "        # This seems to find the correct solution, but is very slow?\n",
    "        t_next = t * np.exp(-w * t) - delta + t\n",
    "        \n",
    "        if np.abs(t_next - t) < 1e-6:\n",
    "            return t_next, i\n",
    "        i += 1\n",
    "        t = t_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bin_search(w, delta, eps=1e-10):\n",
    "    \"\"\"Solve the equation x * exp(-w * x) = delta using binary search by exploiting the \n",
    "    monotonic decrease of the function after the maxima.\"\"\"\n",
    "    \n",
    "    # Hack to access this variable in the f(x)\n",
    "    evals = [0]\n",
    "    \n",
    "    def f(x):\n",
    "        evals[0] += 1\n",
    "        return x * np.exp(-w * x) - delta\n",
    "    \n",
    "    t = 1 / w # Start searching from the maxima.\n",
    "    while f(t) > 0:\n",
    "        t *= 2 # Exponential search for the upper bound\n",
    "    \n",
    "    t_low, t_hi = t / 2, t\n",
    "    t_mid = (t_hi + t_low) / 2\n",
    "    val = f(t_mid)\n",
    "    \n",
    "    while np.abs(val) > eps:\n",
    "        if f(t_mid) > 0:\n",
    "            t_low = t_mid\n",
    "        else:\n",
    "            t_hi = t_mid\n",
    "        t_mid = (t_low + t_hi) / 2\n",
    "        val = f(t_mid)\n",
    "    \n",
    "    return t_mid, evals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w, delta = .0007, 1e-5\n",
    "# tmp = fp(w, delta)\n",
    "tmp2 = bin_search(w, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mean_next_time(c, w, divs=100, delta=1e-6):\n",
    "    \"\"\"Numerically integrate t * f(t) using trapezoidal rule.\"\"\"\n",
    "    t_maxima = 1 / w\n",
    "    t_hi, _ = bin_search(w, delta, eps=1e-10)\n",
    "    \n",
    "    dx = t_hi / divs\n",
    "    x = np.arange(0, t_hi, dx)\n",
    "    \n",
    "    def f(t):\n",
    "        return c * t * np.exp(-w * t - (c / w) * (np.exp(-w * t) - 1))\n",
    "    \n",
    "    y = f(x)\n",
    "    y_min = np.asarray([min(y1, y2) for y1, y2 in zip(y[:-1], y[1:])])\n",
    "    dy_abs = np.abs(np.diff(y))\n",
    "    area = np.sum(0.5 * dx * dy_abs + y_min * dx)\n",
    "    return area\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.022691176265778722"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c, w, divs = 1, 7, 100\n",
    "get_mean_next_time(c, w, divs=divs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.integrate import quad, quad_explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'), (3, 'b')]"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip([1, 3, 4], 'ab'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quad_func(t, c, w):\n",
    "    return c * t * np.exp(-w * t - (c / w) * (np.exp(-w * t) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val, err = quad(quad_func, 0, np.inf, args=(c, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.02272734477725196, 2.1300029236454235e-11)"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(val, err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.91844462e-06,   5.77992082e-07])"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quad_func(2, np.asarray([1, 2]), np.asarray([7, 8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`quad` cannot integrate multivariate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only length-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-459-b943bf373931>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquad_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py\u001b[0m in \u001b[0;36mquad\u001b[0;34m(func, a, b, args, full_output, epsabs, epsrel, limit, points, weight, wvar, wopts, maxp1, limlst)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         retval = _quad(func, a, b, args, full_output, epsabs, epsrel, limit,\n\u001b[0;32m--> 323\u001b[0;31m                        points)\n\u001b[0m\u001b[1;32m    324\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         retval = _quad_weight(func, a, b, args, full_output, epsabs, epsrel,\n",
      "\u001b[0;32m/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py\u001b[0m in \u001b[0;36m_quad\u001b[0;34m(func, a, b, args, full_output, epsabs, epsrel, limit, points)\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_quadpack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qagse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfull_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsabs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsrel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_quadpack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qagie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfbounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfull_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsabs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsrel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minfbounds\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "quad(quad_func, 0, np.inf, args=(np.asarray([1, 2]), np.asarray([7, 8])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Testing package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tf_rmtpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/__init__.py'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_rmtpp.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 890 ms, sys: 60 ms, total: 950 ms\n",
      "Wall time: 955 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import tensorflow as tf\n",
    "path = 'rmtpp-data/real/so/'\n",
    "# # Run: 309\n",
    "idx = 1\n",
    "event_train_file = os.path.join(path, f'event-{idx}-train.txt')\n",
    "event_test_file = os.path.join(path, f'event-{idx}-test.txt')\n",
    "time_train_file = os.path.join(path, f'time-{idx}-train.txt')\n",
    "time_test_file = os.path.join(path, f'time-{idx}-test.txt')\n",
    "batch_size=512\n",
    "bptt=10\n",
    "\n",
    "# path = 'rmtpp-data/synthetic/exp/'\n",
    "# event_train_file = os.path.join(path, f'event-train.txt')\n",
    "# event_test_file = os.path.join(path, f'event-test.txt')\n",
    "# time_train_file = os.path.join(path, f'time-train.txt')\n",
    "# time_test_file = os.path.join(path, f'time-test.txt')\n",
    "# batch_size=16\n",
    "# bptt=5\n",
    "\n",
    "# path = 'rmtpp-data/synthetic/mixture-HMM/'\n",
    "# event_train_file = os.path.join(path, f'event-temporal-3-train.txt')\n",
    "# event_test_file = os.path.join(path, f'event-temporal-3-test.txt')\n",
    "# time_train_file = os.path.join(path, f'time-temporal-3-train.txt')\n",
    "# time_test_file = os.path.join(path, f'time-temporal-3-test.txt')\n",
    "# batch_size=16\n",
    "# bptt=5\n",
    "\n",
    "data = tf_rmtpp.utils.read_data(\n",
    "    event_train_file=event_train_file,\n",
    "    event_test_file=event_test_file,\n",
    "    time_train_file=time_train_file,\n",
    "    time_test_file=time_test_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data['train_time_in_seq'] *= 1000\n",
    "# data['train_time_out_seq'] *= 1000\n",
    "# data['test_time_in_seq'] *= 1000\n",
    "# data['test_time_out_seq'] *= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tf_rmtpp' from '/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/__init__.py'>"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run += 1\n",
    "reload(tf_rmtpp.utils)\n",
    "reload(tf_rmtpp.rmtpp_core)\n",
    "reload(tf_rmtpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "710"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterSession.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "iterSession = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_rate = tf_rmtpp.utils.calc_base_rate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.3332661209504799"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(base_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_class_prob = tf_rmtpp.utils.calc_base_event_prob(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['num_categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.24255704e-02,   1.97711406e-02,   6.48364270e-03,\n",
       "         4.36052758e-01,   5.75456369e-02,   6.17480959e-02,\n",
       "         1.94244642e-02,   1.92365709e-02,   2.29195446e-01,\n",
       "         1.39199839e-02,   2.19914575e-03,   2.74800595e-02,\n",
       "         8.77805829e-03,   1.88898945e-02,   3.03011057e-03,\n",
       "         9.92394290e-04,   2.77341124e-03,   5.37745386e-03,\n",
       "         3.75257361e-03,   7.14523889e-04,   1.50843932e-04,\n",
       "         5.82204650e-05])"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_class_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num categories =  22\n",
      "delta-t (training) = \n",
      "count    3.778740e+05\n",
      "mean     1.312461e-02\n",
      "std      1.652969e-02\n",
      "min      2.058333e-10\n",
      "25%      2.613212e-03\n",
      "50%      7.631856e-03\n",
      "75%      1.736150e-02\n",
      "max      3.220176e-01\n",
      "dtype: float64\n",
      "base-rate = 76.19273530557021, log(base_rate) = 4.33326612095048\n",
      "Class probs =  [  6.24255704e-02   1.97711406e-02   6.48364270e-03   4.36052758e-01\n",
      "   5.75456369e-02   6.17480959e-02   1.94244642e-02   1.92365709e-02\n",
      "   2.29195446e-01   1.39199839e-02   2.19914575e-03   2.74800595e-02\n",
      "   8.77805829e-03   1.88898945e-02   3.03011057e-03   9.92394290e-04\n",
      "   2.77341124e-03   5.37745386e-03   3.75257361e-03   7.14523889e-04\n",
      "   1.50843932e-04   5.82204650e-05]\n",
      "delta-t (testing) = \n",
      "count    9.590700e+04\n",
      "mean     1.291405e-02\n",
      "std      1.646905e-02\n",
      "min      2.058333e-10\n",
      "25%      2.424433e-03\n",
      "50%      7.344151e-03\n",
      "75%      1.706356e-02\n",
      "max      2.712632e-01\n",
      "dtype: float64\n",
      "base-rate = 77.43506973871634, log(base_rate) = 4.349439775377487\n",
      "Class probs =  [  6.24667647e-02   2.13435933e-02   7.06934843e-03   4.30688062e-01\n",
      "   5.79936814e-02   6.49587621e-02   2.00298206e-02   1.87264746e-02\n",
      "   2.28523465e-01   1.27832171e-02   2.03321968e-03   2.71617296e-02\n",
      "   8.39354792e-03   2.01862221e-02   3.13845705e-03   1.04267676e-03\n",
      "   2.79437372e-03   5.89112369e-03   3.86833078e-03   6.04752521e-04\n",
      "   2.29388887e-04   7.29873732e-05]\n",
      "Training sequence lenghts = \n",
      "count    5307.000000\n",
      "mean       71.202940\n",
      "std        42.446415\n",
      "min        40.000000\n",
      "25%        46.000000\n",
      "50%        57.000000\n",
      "75%        79.000000\n",
      "max       719.000000\n",
      "dtype: float64\n",
      "Testing sequence lenghts = \n",
      "count    1326.000000\n",
      "mean       72.328054\n",
      "std        48.312648\n",
      "min        40.000000\n",
      "25%        46.000000\n",
      "50%        57.000000\n",
      "75%        79.000000\n",
      "max       735.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "tf_rmtpp.utils.data_stats(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "rmtpp_mdl = tf_rmtpp.rmtpp_core.RMTPP(\n",
    "    sess=iterSession, \n",
    "    num_categories=data['num_categories'],\n",
    "    summary_dir='./summary.rmtpp.2/train-{}/'.format(run),\n",
    "#     summary_dir='./summary.rmtpp/train-306/',\n",
    "    bt=np.log(base_rate),\n",
    "    decay_rates=0.001,\n",
    "    decay_steps=10,\n",
    "    bptt=bptt,\n",
    "    batch_size=batch_size,\n",
    "    _opts=tf_rmtpp.def_opts,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 187 ms, sys: 13.3 ms, total: 200 ms\n",
      "Wall time: 182 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rmtpp_mdl.initialize(finalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rmtpp_mdl.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch... 0\n",
      "Loss during batch 0 last BPTT = -0.612, lr = 0.09961\n",
      "Loss on last epoch = 3.8985, new lr = 0.09575, global_step = 444\n",
      "Model saved at ./save.rmtpp/model.ckpt\n",
      "Running evaluation on training data: ...\n",
      "** MAE = 0.017; valid = 6536, ACC = 0.438\n",
      "CPU times: user 25.8 s, sys: 5.67 s, total: 31.4 s\n",
      "Wall time: 26.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rmtpp_mdl.train(training_data=data, restart=False, \n",
    "                with_summaries=True, num_epochs=1, with_evals=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring from ./save.rmtpp/model.ckpt-444\n",
      "INFO:tensorflow:Restoring parameters from ./save.rmtpp/model.ckpt-444\n",
      "Starting epoch... 1\n",
      "Loss during batch 0 last BPTT = 5.095, lr = 0.09538\n",
      "Loss on last epoch = 7.6198, new lr = 0.09197, global_step = 873\n",
      "Starting epoch... 2\n",
      "Loss during batch 0 last BPTT = 0.234, lr = 0.09158\n",
      "Loss on last epoch = 4.5893, new lr = 0.08857, global_step = 1291\n",
      "Starting epoch... 3\n",
      "Loss during batch 0 last BPTT = 9.034, lr = 0.08818\n",
      "Loss on last epoch = 5.1656, new lr = 0.08532, global_step = 1720\n",
      "Starting epoch... 4\n",
      "Loss during batch 0 last BPTT = 1.038, lr = 0.08494\n",
      "Loss on last epoch = 4.2819, new lr = 0.08239, global_step = 2137\n",
      "Starting epoch... 5\n",
      "Loss during batch 0 last BPTT = 3.531, lr = 0.08191\n",
      "Loss on last epoch = 3.2640, new lr = 0.07960, global_step = 2563\n",
      "Starting epoch... 6\n",
      "Loss during batch 0 last BPTT = 0.800, lr = 0.07941\n",
      "Loss on last epoch = 2.5552, new lr = 0.07695, global_step = 2995\n",
      "Starting epoch... 7\n",
      "Loss during batch 0 last BPTT = 1.327, lr = 0.07672\n",
      "Loss on last epoch = 3.4987, new lr = 0.07438, global_step = 3444\n",
      "Starting epoch... 8\n",
      "Loss during batch 0 last BPTT = 0.685, lr = 0.07419\n",
      "Loss on last epoch = 2.2886, new lr = 0.07207, global_step = 3876\n",
      "Starting epoch... 9\n",
      "Loss during batch 0 last BPTT = 1.072, lr = 0.07189\n",
      "Loss on last epoch = 1.6257, new lr = 0.06981, global_step = 4325\n",
      "Starting epoch... 10\n",
      "Loss during batch 0 last BPTT = 5.988, lr = 0.06961\n",
      "Loss on last epoch = 4.0390, new lr = 0.06769, global_step = 4773\n",
      "Starting epoch... 11\n",
      "Loss during batch 0 last BPTT = 1.632, lr = 0.06756\n",
      "Loss on last epoch = 0.3587, new lr = 0.06577, global_step = 5204\n",
      "Starting epoch... 12\n",
      "Loss during batch 0 last BPTT = 1.732, lr = 0.06564\n",
      "Loss on last epoch = 1.5196, new lr = 0.06411, global_step = 5598\n",
      "Starting epoch... 13\n",
      "Loss during batch 0 last BPTT = 0.409, lr = 0.06397\n",
      "Loss on last epoch = 0.8994, new lr = 0.06243, global_step = 6019\n",
      "Starting epoch... 14\n",
      "Loss during batch 0 last BPTT = 0.617, lr = 0.06222\n",
      "Loss on last epoch = 1.0927, new lr = 0.06083, global_step = 6440\n",
      "Starting epoch... 15\n",
      "Loss during batch 0 last BPTT = 1.344, lr = 0.06057\n",
      "Loss on last epoch = 1.3976, new lr = 0.05923, global_step = 6882\n",
      "Starting epoch... 16\n",
      "Loss during batch 0 last BPTT = 2.614, lr = 0.05907\n",
      "Loss on last epoch = 0.3238, new lr = 0.05784, global_step = 7288\n",
      "Starting epoch... 17\n",
      "Loss during batch 0 last BPTT = -0.439, lr = 0.05771\n",
      "Loss on last epoch = -0.1034, new lr = 0.05640, global_step = 7731\n",
      "Starting epoch... 18\n",
      "Loss during batch 0 last BPTT = -0.869, lr = 0.05629\n",
      "Loss on last epoch = 1.1130, new lr = 0.05512, global_step = 8143\n",
      "Starting epoch... 19\n",
      "Loss during batch 0 last BPTT = -0.397, lr = 0.05500\n",
      "Loss on last epoch = 0.4796, new lr = 0.05382, global_step = 8582\n",
      "Starting epoch... 20\n",
      "Loss during batch 0 last BPTT = -0.942, lr = 0.05368\n",
      "Loss on last epoch = 0.0229, new lr = 0.05261, global_step = 9009\n",
      "Starting epoch... 21\n",
      "Loss during batch 0 last BPTT = 1.242, lr = 0.05250\n",
      "Loss on last epoch = 0.7144, new lr = 0.05143, global_step = 9443\n",
      "Starting epoch... 22\n",
      "Loss during batch 0 last BPTT = 0.555, lr = 0.05135\n",
      "Loss on last epoch = 0.2271, new lr = 0.05036, global_step = 9858\n",
      "Starting epoch... 23\n",
      "Loss during batch 0 last BPTT = 0.475, lr = 0.05018\n",
      "Loss on last epoch = 0.5273, new lr = 0.04928, global_step = 10293\n",
      "Starting epoch... 24\n",
      "Loss during batch 0 last BPTT = -0.612, lr = 0.04919\n",
      "Loss on last epoch = 0.4214, new lr = 0.04825, global_step = 10725\n",
      "Starting epoch... 25\n",
      "Loss during batch 0 last BPTT = -0.920, lr = 0.04809\n",
      "Loss on last epoch = 0.2124, new lr = 0.04730, global_step = 11142\n",
      "Starting epoch... 26\n",
      "Loss during batch 0 last BPTT = -0.169, lr = 0.04722\n",
      "Loss on last epoch = 0.4520, new lr = 0.04630, global_step = 11596\n",
      "Starting epoch... 27\n",
      "Loss during batch 0 last BPTT = 1.231, lr = 0.04622\n",
      "Loss on last epoch = 0.2204, new lr = 0.04544, global_step = 12006\n",
      "Starting epoch... 28\n",
      "Loss during batch 0 last BPTT = -0.953, lr = 0.04534\n",
      "Loss on last epoch = 0.1678, new lr = 0.04456, global_step = 12441\n",
      "Starting epoch... 29\n",
      "Loss during batch 0 last BPTT = -1.214, lr = 0.04450\n",
      "Loss on last epoch = -0.3644, new lr = 0.04370, global_step = 12882\n",
      "Starting epoch... 30\n",
      "Loss during batch 0 last BPTT = 0.196, lr = 0.04363\n",
      "Loss on last epoch = 0.2757, new lr = 0.04288, global_step = 13319\n",
      "Starting epoch... 31\n",
      "Loss during batch 0 last BPTT = -0.626, lr = 0.04281\n",
      "Loss on last epoch = 0.0182, new lr = 0.04209, global_step = 13761\n",
      "Starting epoch... 32\n",
      "Loss during batch 0 last BPTT = 0.639, lr = 0.04199\n",
      "Loss on last epoch = 0.1618, new lr = 0.04132, global_step = 14201\n",
      "Starting epoch... 33\n",
      "Loss during batch 0 last BPTT = -1.307, lr = 0.04123\n",
      "Loss on last epoch = 0.0150, new lr = 0.04062, global_step = 14616\n",
      "Starting epoch... 34\n",
      "Loss during batch 0 last BPTT = -0.273, lr = 0.04057\n",
      "Loss on last epoch = 0.0270, new lr = 0.03992, global_step = 15047\n",
      "Starting epoch... 35\n",
      "Loss during batch 0 last BPTT = 0.067, lr = 0.03988\n",
      "Loss on last epoch = 0.3927, new lr = 0.03927, global_step = 15467\n",
      "Starting epoch... 36\n",
      "Loss during batch 0 last BPTT = 1.568, lr = 0.03922\n",
      "Loss on last epoch = -0.0586, new lr = 0.03866, global_step = 15868\n",
      "Starting epoch... 37\n",
      "Loss during batch 0 last BPTT = -1.056, lr = 0.03858\n",
      "Loss on last epoch = -0.0273, new lr = 0.03802, global_step = 16302\n",
      "Starting epoch... 38\n",
      "Loss during batch 0 last BPTT = 0.031, lr = 0.03794\n",
      "Loss on last epoch = 0.1329, new lr = 0.03741, global_step = 16728\n",
      "Starting epoch... 39\n",
      "Loss during batch 0 last BPTT = -1.153, lr = 0.03735\n",
      "Loss on last epoch = -0.4500, new lr = 0.03680, global_step = 17173\n",
      "Starting epoch... 40\n",
      "Loss during batch 0 last BPTT = 1.386, lr = 0.03675\n",
      "Loss on last epoch = 0.0231, new lr = 0.03621, global_step = 17616\n",
      "Starting epoch... 41\n",
      "Loss during batch 0 last BPTT = -0.215, lr = 0.03617\n",
      "Loss on last epoch = -0.1914, new lr = 0.03568, global_step = 18029\n",
      "Starting epoch... 42\n",
      "Loss during batch 0 last BPTT = -0.944, lr = 0.03561\n",
      "Loss on last epoch = -0.5330, new lr = 0.03515, global_step = 18451\n",
      "Starting epoch... 43\n",
      "Loss during batch 0 last BPTT = -0.776, lr = 0.03508\n",
      "Loss on last epoch = -0.6845, new lr = 0.03464, global_step = 18867\n",
      "Starting epoch... 44\n",
      "Loss during batch 0 last BPTT = -0.882, lr = 0.03459\n",
      "Loss on last epoch = 0.2043, new lr = 0.03414, global_step = 19290\n",
      "Starting epoch... 45\n",
      "Loss during batch 0 last BPTT = -0.117, lr = 0.03411\n",
      "Loss on last epoch = -0.3226, new lr = 0.03365, global_step = 19716\n",
      "Starting epoch... 46\n",
      "Loss during batch 0 last BPTT = -0.736, lr = 0.03357\n",
      "Loss on last epoch = 0.2468, new lr = 0.03316, global_step = 20161\n",
      "Starting epoch... 47\n",
      "Loss during batch 0 last BPTT = 1.267, lr = 0.03311\n",
      "Loss on last epoch = -0.3484, new lr = 0.03267, global_step = 20612\n",
      "Starting epoch... 48\n",
      "Loss during batch 0 last BPTT = -1.214, lr = 0.03263\n",
      "Loss on last epoch = -0.6000, new lr = 0.03222, global_step = 21041\n",
      "Starting epoch... 49\n",
      "Loss during batch 0 last BPTT = -0.579, lr = 0.03218\n",
      "Loss on last epoch = -0.2784, new lr = 0.03179, global_step = 21454\n",
      "Starting epoch... 50\n",
      "Loss during batch 0 last BPTT = -1.142, lr = 0.03175\n",
      "Loss on last epoch = -0.4065, new lr = 0.03139, global_step = 21859\n",
      "Starting epoch... 51\n",
      "Loss during batch 0 last BPTT = -1.370, lr = 0.03135\n",
      "Loss on last epoch = -0.7145, new lr = 0.03100, global_step = 22257\n",
      "Starting epoch... 52\n",
      "Loss during batch 0 last BPTT = -1.251, lr = 0.03097\n",
      "Loss on last epoch = -0.5121, new lr = 0.03062, global_step = 22661\n",
      "Starting epoch... 53\n",
      "Loss during batch 0 last BPTT = -0.519, lr = 0.03055\n",
      "Loss on last epoch = -0.1933, new lr = 0.03022, global_step = 23095\n",
      "Starting epoch... 54\n",
      "Loss during batch 0 last BPTT = -1.181, lr = 0.03018\n",
      "Loss on last epoch = -0.7386, new lr = 0.02984, global_step = 23512\n",
      "Starting epoch... 55\n",
      "Loss during batch 0 last BPTT = -1.024, lr = 0.02978\n",
      "Loss on last epoch = -0.4066, new lr = 0.02946, global_step = 23950\n",
      "Starting epoch... 56\n",
      "Loss during batch 0 last BPTT = -0.125, lr = 0.02942\n",
      "Loss on last epoch = -0.5578, new lr = 0.02907, global_step = 24396\n",
      "Starting epoch... 57\n",
      "Loss during batch 0 last BPTT = -1.174, lr = 0.02903\n",
      "Loss on last epoch = -0.4676, new lr = 0.02871, global_step = 24833\n",
      "Starting epoch... 58\n",
      "Loss during batch 0 last BPTT = -1.410, lr = 0.02868\n",
      "Loss on last epoch = -0.4687, new lr = 0.02834, global_step = 25282\n",
      "Starting epoch... 59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss during batch 0 last BPTT = -1.242, lr = 0.02830\n",
      "Loss on last epoch = -0.9622, new lr = 0.02800, global_step = 25718\n",
      "Starting epoch... 60\n",
      "Loss during batch 0 last BPTT = 1.278, lr = 0.02797\n",
      "Loss on last epoch = -0.4629, new lr = 0.02766, global_step = 26150\n",
      "Starting epoch... 61\n",
      "Loss during batch 0 last BPTT = -0.903, lr = 0.02763\n",
      "Loss on last epoch = -0.8746, new lr = 0.02733, global_step = 26594\n",
      "Starting epoch... 62\n",
      "Loss during batch 0 last BPTT = -0.038, lr = 0.02730\n",
      "Loss on last epoch = -0.4308, new lr = 0.02702, global_step = 27015\n",
      "Starting epoch... 63\n",
      "Loss during batch 0 last BPTT = -0.086, lr = 0.02698\n",
      "Loss on last epoch = -0.5268, new lr = 0.02670, global_step = 27453\n",
      "Starting epoch... 64\n",
      "Loss during batch 0 last BPTT = -0.382, lr = 0.02666\n",
      "Loss on last epoch = -0.3949, new lr = 0.02639, global_step = 27895\n",
      "Starting epoch... 65\n",
      "Loss during batch 0 last BPTT = -0.380, lr = 0.02635\n",
      "Loss on last epoch = -0.6027, new lr = 0.02610, global_step = 28310\n",
      "Starting epoch... 66\n",
      "Loss during batch 0 last BPTT = -0.489, lr = 0.02607\n",
      "Loss on last epoch = -0.3734, new lr = 0.02581, global_step = 28748\n",
      "Starting epoch... 67\n",
      "Loss during batch 0 last BPTT = -1.126, lr = 0.02576\n",
      "Loss on last epoch = -0.6990, new lr = 0.02552, global_step = 29186\n",
      "Starting epoch... 68\n",
      "Loss during batch 0 last BPTT = -0.714, lr = 0.02550\n",
      "Loss on last epoch = -0.5878, new lr = 0.02526, global_step = 29596\n",
      "Starting epoch... 69\n",
      "Loss during batch 0 last BPTT = -0.711, lr = 0.02522\n",
      "Loss on last epoch = -0.5855, new lr = 0.02498, global_step = 30028\n",
      "Starting epoch... 70\n",
      "Loss during batch 0 last BPTT = -0.481, lr = 0.02495\n",
      "Loss on last epoch = -0.5376, new lr = 0.02471, global_step = 30470\n",
      "Starting epoch... 71\n",
      "Loss during batch 0 last BPTT = -0.520, lr = 0.02469\n",
      "Loss on last epoch = -0.5967, new lr = 0.02445, global_step = 30903\n",
      "Starting epoch... 72\n",
      "Loss during batch 0 last BPTT = -0.634, lr = 0.02442\n",
      "Loss on last epoch = -0.5257, new lr = 0.02419, global_step = 31334\n",
      "Starting epoch... 73\n",
      "Loss during batch 0 last BPTT = -0.768, lr = 0.02417\n",
      "Loss on last epoch = -0.6989, new lr = 0.02394, global_step = 31771\n",
      "Starting epoch... 74\n",
      "Loss during batch 0 last BPTT = -0.755, lr = 0.02392\n",
      "Loss on last epoch = -0.6145, new lr = 0.02370, global_step = 32187\n",
      "Starting epoch... 75\n",
      "Loss during batch 0 last BPTT = -0.857, lr = 0.02366\n",
      "Loss on last epoch = -0.6767, new lr = 0.02347, global_step = 32606\n",
      "Starting epoch... 76\n",
      "Loss during batch 0 last BPTT = -0.717, lr = 0.02345\n",
      "Loss on last epoch = -0.7013, new lr = 0.02324, global_step = 33036\n",
      "Starting epoch... 77\n",
      "Loss during batch 0 last BPTT = -0.509, lr = 0.02322\n",
      "Loss on last epoch = -0.5705, new lr = 0.02301, global_step = 33455\n",
      "Starting epoch... 78\n",
      "Loss during batch 0 last BPTT = -1.158, lr = 0.02300\n",
      "Loss on last epoch = -0.8472, new lr = 0.02280, global_step = 33859\n",
      "Starting epoch... 79\n",
      "Loss during batch 0 last BPTT = -0.423, lr = 0.02277\n",
      "Loss on last epoch = -0.5841, new lr = 0.02258, global_step = 34296\n",
      "Starting epoch... 80\n",
      "Loss during batch 0 last BPTT = -1.001, lr = 0.02256\n",
      "Loss on last epoch = -0.6825, new lr = 0.02236, global_step = 34721\n",
      "Starting epoch... 81\n",
      "Loss during batch 0 last BPTT = -1.321, lr = 0.02233\n",
      "Loss on last epoch = -0.9361, new lr = 0.02215, global_step = 35150\n",
      "Starting epoch... 82\n",
      "Loss during batch 0 last BPTT = -0.658, lr = 0.02213\n",
      "Loss on last epoch = -0.6319, new lr = 0.02193, global_step = 35602\n",
      "Starting epoch... 83\n",
      "Loss during batch 0 last BPTT = -0.434, lr = 0.02192\n",
      "Loss on last epoch = -0.6346, new lr = 0.02172, global_step = 36040\n",
      "Starting epoch... 84\n",
      "Loss during batch 0 last BPTT = -1.131, lr = 0.02170\n",
      "Loss on last epoch = -0.8178, new lr = 0.02152, global_step = 36470\n",
      "Starting epoch... 85\n",
      "Loss during batch 0 last BPTT = -0.840, lr = 0.02150\n",
      "Loss on last epoch = -0.7956, new lr = 0.02132, global_step = 36898\n",
      "Starting epoch... 86\n",
      "Loss during batch 0 last BPTT = -0.826, lr = 0.02130\n",
      "Loss on last epoch = -0.7241, new lr = 0.02113, global_step = 37335\n",
      "Starting epoch... 87\n",
      "Loss during batch 0 last BPTT = -0.760, lr = 0.02111\n",
      "Loss on last epoch = -0.6900, new lr = 0.02094, global_step = 37759\n",
      "Starting epoch... 88\n",
      "Loss during batch 0 last BPTT = -1.150, lr = 0.02091\n",
      "Loss on last epoch = -0.8515, new lr = 0.02075, global_step = 38202\n",
      "Starting epoch... 89\n",
      "Loss during batch 0 last BPTT = -0.695, lr = 0.02073\n",
      "Loss on last epoch = -0.7138, new lr = 0.02056, global_step = 38632\n",
      "Starting epoch... 90\n",
      "Loss during batch 0 last BPTT = -1.041, lr = 0.02055\n",
      "Loss on last epoch = -0.8511, new lr = 0.02039, global_step = 39054\n",
      "Starting epoch... 91\n",
      "Loss during batch 0 last BPTT = -0.602, lr = 0.02037\n",
      "Loss on last epoch = -0.8205, new lr = 0.02022, global_step = 39458\n",
      "Starting epoch... 92\n",
      "Loss during batch 0 last BPTT = -0.983, lr = 0.02020\n",
      "Loss on last epoch = -0.8694, new lr = 0.02004, global_step = 39896\n",
      "Starting epoch... 93\n",
      "Loss during batch 0 last BPTT = -0.820, lr = 0.02003\n",
      "Loss on last epoch = -0.8508, new lr = 0.01987, global_step = 40336\n",
      "Starting epoch... 94\n",
      "Loss during batch 0 last BPTT = -1.012, lr = 0.01985\n",
      "Loss on last epoch = -0.9374, new lr = 0.01970, global_step = 40751\n",
      "Starting epoch... 95\n",
      "Loss during batch 0 last BPTT = -0.747, lr = 0.01969\n",
      "Loss on last epoch = -0.7851, new lr = 0.01953, global_step = 41192\n",
      "Starting epoch... 96\n",
      "Loss during batch 0 last BPTT = -0.987, lr = 0.01952\n",
      "Loss on last epoch = -0.8600, new lr = 0.01937, global_step = 41627\n",
      "Starting epoch... 97\n",
      "Loss during batch 0 last BPTT = -1.048, lr = 0.01936\n",
      "Loss on last epoch = -0.8741, new lr = 0.01920, global_step = 42078\n",
      "Starting epoch... 98\n",
      "Loss during batch 0 last BPTT = -0.733, lr = 0.01919\n",
      "Loss on last epoch = -0.8294, new lr = 0.01905, global_step = 42500\n",
      "Starting epoch... 99\n",
      "Loss during batch 0 last BPTT = -0.783, lr = 0.01903\n",
      "Loss on last epoch = -0.8072, new lr = 0.01889, global_step = 42943\n",
      "Starting epoch... 100\n",
      "Loss during batch 0 last BPTT = -1.050, lr = 0.01888\n",
      "Loss on last epoch = -0.8880, new lr = 0.01874, global_step = 43364\n",
      "Model saved at ./save.rmtpp/model.ckpt\n",
      "Running evaluation on training data: ...\n",
      "** MAE = 0.019; valid = 43, ACC = 0.209\n",
      "CPU times: user 34min 35s, sys: 3min 49s, total: 38min 24s\n",
      "Wall time: 15min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rmtpp_mdl.train(training_data=data, restart=True, \n",
    "                with_summaries=True, num_epochs=100, with_evals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring from ./save.rmtpp/model.ckpt-43364\n",
      "INFO:tensorflow:Restoring parameters from ./save.rmtpp/model.ckpt-43364\n",
      "Starting epoch... 101\n",
      "Loss during batch 0 last BPTT = -0.842, lr = 0.01873\n",
      "Loss on last epoch = -0.8392, new lr = 0.01859, global_step = 43795\n",
      "Starting epoch... 102\n",
      "Loss during batch 0 last BPTT = -0.728, lr = 0.01856\n",
      "Loss on last epoch = -0.8609, new lr = 0.01845, global_step = 44196\n",
      "Starting epoch... 103\n",
      "Loss during batch 0 last BPTT = -1.271, lr = 0.01843\n",
      "Loss on last epoch = -0.9111, new lr = 0.01830, global_step = 44631\n",
      "Starting epoch... 104\n",
      "Loss during batch 0 last BPTT = -0.725, lr = 0.01829\n",
      "Loss on last epoch = -0.7901, new lr = 0.01816, global_step = 45055\n",
      "Starting epoch... 105\n",
      "Loss during batch 0 last BPTT = -0.865, lr = 0.01815\n",
      "Loss on last epoch = -0.8249, new lr = 0.01802, global_step = 45489\n",
      "Starting epoch... 106\n",
      "Loss during batch 0 last BPTT = -0.822, lr = 0.01801\n",
      "Loss on last epoch = -0.9185, new lr = 0.01789, global_step = 45894\n",
      "Starting epoch... 107\n",
      "Loss during batch 0 last BPTT = -0.784, lr = 0.01788\n",
      "Loss on last epoch = -0.8217, new lr = 0.01775, global_step = 46339\n",
      "Starting epoch... 108\n",
      "Loss during batch 0 last BPTT = -0.821, lr = 0.01773\n",
      "Loss on last epoch = -0.8918, new lr = 0.01761, global_step = 46782\n",
      "Starting epoch... 109\n",
      "Loss during batch 0 last BPTT = -0.850, lr = 0.01759\n",
      "Loss on last epoch = -0.8791, new lr = 0.01748, global_step = 47207\n",
      "Starting epoch... 110\n",
      "Loss during batch 0 last BPTT = -0.982, lr = 0.01747\n",
      "Loss on last epoch = -0.9741, new lr = 0.01736, global_step = 47614\n",
      "Starting epoch... 111\n",
      "Loss during batch 0 last BPTT = -0.881, lr = 0.01734\n",
      "Loss on last epoch = -0.8344, new lr = 0.01722, global_step = 48061\n",
      "Starting epoch... 112\n",
      "Loss during batch 0 last BPTT = -0.937, lr = 0.01721\n",
      "Loss on last epoch = -0.9317, new lr = 0.01710, global_step = 48480\n",
      "Starting epoch... 113\n",
      "Loss during batch 0 last BPTT = -0.830, lr = 0.01709\n",
      "Loss on last epoch = -0.9243, new lr = 0.01697, global_step = 48914\n",
      "Starting epoch... 114\n",
      "Loss during batch 0 last BPTT = -0.836, lr = 0.01696\n",
      "Loss on last epoch = -0.8672, new lr = 0.01685, global_step = 49363\n",
      "Starting epoch... 115\n",
      "Loss during batch 0 last BPTT = -0.937, lr = 0.01683\n",
      "Loss on last epoch = -0.9837, new lr = 0.01673, global_step = 49788\n",
      "Starting epoch... 116\n",
      "Loss during batch 0 last BPTT = -0.858, lr = 0.01672\n",
      "Loss on last epoch = -0.9362, new lr = 0.01661, global_step = 50216\n",
      "Starting epoch... 117\n",
      "Loss during batch 0 last BPTT = -0.787, lr = 0.01660\n",
      "Loss on last epoch = -0.9141, new lr = 0.01649, global_step = 50652\n",
      "Starting epoch... 118\n",
      "Loss during batch 0 last BPTT = -0.930, lr = 0.01648\n",
      "Loss on last epoch = -0.9149, new lr = 0.01638, global_step = 51061\n",
      "Starting epoch... 119\n",
      "Loss during batch 0 last BPTT = -1.134, lr = 0.01637\n",
      "Loss on last epoch = -0.9654, new lr = 0.01626, global_step = 51486\n",
      "Starting epoch... 120\n",
      "Loss during batch 0 last BPTT = -0.861, lr = 0.01625\n",
      "Loss on last epoch = -0.9600, new lr = 0.01615, global_step = 51911\n",
      "Starting epoch... 121\n",
      "Loss during batch 0 last BPTT = -1.059, lr = 0.01614\n",
      "Loss on last epoch = -0.9618, new lr = 0.01604, global_step = 52340\n",
      "Starting epoch... 122\n",
      "Loss during batch 0 last BPTT = -0.911, lr = 0.01603\n",
      "Loss on last epoch = -0.9658, new lr = 0.01593, global_step = 52791\n",
      "Starting epoch... 123\n",
      "Loss during batch 0 last BPTT = -0.865, lr = 0.01592\n",
      "Loss on last epoch = -0.9956, new lr = 0.01582, global_step = 53193\n",
      "Starting epoch... 124\n",
      "Loss during batch 0 last BPTT = -0.987, lr = 0.01582\n",
      "Loss on last epoch = -0.9356, new lr = 0.01571, global_step = 53640\n",
      "Starting epoch... 125\n",
      "Loss during batch 0 last BPTT = -1.022, lr = 0.01570\n",
      "Loss on last epoch = -0.9898, new lr = 0.01561, global_step = 54065\n",
      "Starting epoch... 126\n",
      "Loss during batch 0 last BPTT = -0.932, lr = 0.01560\n",
      "Loss on last epoch = -0.9913, new lr = 0.01551, global_step = 54489\n",
      "Starting epoch... 127\n",
      "Loss during batch 0 last BPTT = -0.823, lr = 0.01550\n",
      "Loss on last epoch = -0.9448, new lr = 0.01540, global_step = 54918\n",
      "Starting epoch... 128\n",
      "Loss during batch 0 last BPTT = -0.856, lr = 0.01540\n",
      "Loss on last epoch = -0.9832, new lr = 0.01530, global_step = 55351\n",
      "Starting epoch... 129\n",
      "Loss during batch 0 last BPTT = -0.945, lr = 0.01529\n",
      "Loss on last epoch = -0.9571, new lr = 0.01520, global_step = 55804\n",
      "Starting epoch... 130\n",
      "Loss during batch 0 last BPTT = -0.946, lr = 0.01519\n",
      "Loss on last epoch = -1.0437, new lr = 0.01510, global_step = 56213\n",
      "Starting epoch... 131\n",
      "Loss during batch 0 last BPTT = -0.891, lr = 0.01509\n",
      "Loss on last epoch = -0.9433, new lr = 0.01500, global_step = 56651\n",
      "Starting epoch... 132\n",
      "Loss during batch 0 last BPTT = -1.038, lr = 0.01499\n",
      "Loss on last epoch = -1.0450, new lr = 0.01491, global_step = 57076\n",
      "Starting epoch... 133\n",
      "Loss during batch 0 last BPTT = -0.985, lr = 0.01490\n",
      "Loss on last epoch = -0.9651, new lr = 0.01481, global_step = 57511\n",
      "Starting epoch... 134\n",
      "Loss during batch 0 last BPTT = -1.157, lr = 0.01480\n",
      "Loss on last epoch = -1.0987, new lr = 0.01473, global_step = 57906\n",
      "Starting epoch... 135\n",
      "Loss during batch 0 last BPTT = -1.044, lr = 0.01472\n",
      "Loss on last epoch = -1.0954, new lr = 0.01464, global_step = 58316\n",
      "Starting epoch... 136\n",
      "Loss during batch 0 last BPTT = -0.952, lr = 0.01463\n",
      "Loss on last epoch = -0.9842, new lr = 0.01455, global_step = 58739\n",
      "Starting epoch... 137\n",
      "Loss during batch 0 last BPTT = -1.033, lr = 0.01454\n",
      "Loss on last epoch = -1.0429, new lr = 0.01445, global_step = 59187\n",
      "Starting epoch... 138\n",
      "Loss during batch 0 last BPTT = -0.846, lr = 0.01444\n",
      "Loss on last epoch = -0.9939, new lr = 0.01436, global_step = 59620\n",
      "Starting epoch... 139\n",
      "Loss during batch 0 last BPTT = -0.992, lr = 0.01436\n",
      "Loss on last epoch = -1.0243, new lr = 0.01427, global_step = 60057\n",
      "Starting epoch... 140\n",
      "Loss during batch 0 last BPTT = -1.010, lr = 0.01427\n",
      "Loss on last epoch = -1.0567, new lr = 0.01419, global_step = 60477\n",
      "Starting epoch... 141\n",
      "Loss during batch 0 last BPTT = -0.966, lr = 0.01418\n",
      "Loss on last epoch = -1.0388, new lr = 0.01410, global_step = 60904\n",
      "Starting epoch... 142\n",
      "Loss during batch 0 last BPTT = -1.128, lr = 0.01409\n",
      "Loss on last epoch = -1.1179, new lr = 0.01402, global_step = 61338\n",
      "Starting epoch... 143\n",
      "Loss during batch 0 last BPTT = -1.096, lr = 0.01401\n",
      "Loss on last epoch = -1.0247, new lr = 0.01393, global_step = 61769\n",
      "Starting epoch... 144\n",
      "Loss during batch 0 last BPTT = -1.225, lr = 0.01392\n",
      "Loss on last epoch = -1.1635, new lr = 0.01386, global_step = 62168\n",
      "Starting epoch... 145\n",
      "Loss during batch 0 last BPTT = -1.220, lr = 0.01385\n",
      "Loss on last epoch = -1.0868, new lr = 0.01377, global_step = 62598\n",
      "Starting epoch... 146\n",
      "Loss during batch 0 last BPTT = -1.120, lr = 0.01377\n",
      "Loss on last epoch = -1.0955, new lr = 0.01369, global_step = 63034\n",
      "Starting epoch... 147\n",
      "Loss during batch 0 last BPTT = -0.995, lr = 0.01369\n",
      "Loss on last epoch = -1.0651, new lr = 0.01361, global_step = 63467\n",
      "Starting epoch... 148\n",
      "Loss during batch 0 last BPTT = -1.130, lr = 0.01360\n",
      "Loss on last epoch = -1.1033, new lr = 0.01353, global_step = 63912\n",
      "Starting epoch... 149\n",
      "Loss during batch 0 last BPTT = -1.057, lr = 0.01352\n",
      "Loss on last epoch = -1.0800, new lr = 0.01345, global_step = 64322\n",
      "Starting epoch... 150\n",
      "Loss during batch 0 last BPTT = -0.922, lr = 0.01345\n",
      "Loss on last epoch = -1.1066, new lr = 0.01338, global_step = 64730\n",
      "Starting epoch... 151\n",
      "Loss during batch 0 last BPTT = -1.184, lr = 0.01337\n",
      "Loss on last epoch = -1.1351, new lr = 0.01331, global_step = 65155\n",
      "Starting epoch... 152\n",
      "Loss during batch 0 last BPTT = -1.293, lr = 0.01330\n",
      "Loss on last epoch = -1.1486, new lr = 0.01323, global_step = 65566\n",
      "Starting epoch... 153\n",
      "Loss during batch 0 last BPTT = -1.076, lr = 0.01323\n",
      "Loss on last epoch = -1.1316, new lr = 0.01316, global_step = 65971\n",
      "Starting epoch... 154\n",
      "Loss during batch 0 last BPTT = -1.131, lr = 0.01316\n",
      "Loss on last epoch = -1.1352, new lr = 0.01309, global_step = 66398\n",
      "Starting epoch... 155\n",
      "Loss during batch 0 last BPTT = -1.143, lr = 0.01308\n",
      "Loss on last epoch = -1.1413, new lr = 0.01302, global_step = 66822\n",
      "Starting epoch... 156\n",
      "Loss during batch 0 last BPTT = -1.052, lr = 0.01301\n",
      "Loss on last epoch = -1.1529, new lr = 0.01295, global_step = 67239\n",
      "Starting epoch... 157\n",
      "Loss during batch 0 last BPTT = -1.043, lr = 0.01294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on last epoch = -1.0996, new lr = 0.01287, global_step = 67687\n",
      "Starting epoch... 158\n",
      "Loss during batch 0 last BPTT = -1.005, lr = 0.01287\n",
      "Loss on last epoch = -1.1516, new lr = 0.01280, global_step = 68117\n",
      "Starting epoch... 159\n",
      "Loss during batch 0 last BPTT = -1.116, lr = 0.01279\n",
      "Loss on last epoch = -1.1429, new lr = 0.01273, global_step = 68542\n",
      "Starting epoch... 160\n",
      "Loss during batch 0 last BPTT = -1.154, lr = 0.01272\n",
      "Loss on last epoch = -1.1808, new lr = 0.01266, global_step = 68973\n",
      "Starting epoch... 161\n",
      "Loss during batch 0 last BPTT = -0.960, lr = 0.01266\n",
      "Loss on last epoch = -1.0701, new lr = 0.01259, global_step = 69408\n",
      "Starting epoch... 162\n",
      "Loss during batch 0 last BPTT = -1.074, lr = 0.01259\n",
      "Loss on last epoch = -1.1453, new lr = 0.01253, global_step = 69831\n",
      "Starting epoch... 163\n",
      "Loss during batch 0 last BPTT = -1.058, lr = 0.01252\n",
      "Loss on last epoch = -1.1216, new lr = 0.01246, global_step = 70265\n",
      "Starting epoch... 164\n",
      "Loss during batch 0 last BPTT = -1.091, lr = 0.01245\n",
      "Loss on last epoch = -1.1773, new lr = 0.01240, global_step = 70663\n",
      "Starting epoch... 165\n",
      "Loss during batch 0 last BPTT = -0.957, lr = 0.01239\n",
      "Loss on last epoch = -1.1431, new lr = 0.01233, global_step = 71076\n",
      "Starting epoch... 166\n",
      "Loss during batch 0 last BPTT = -1.170, lr = 0.01233\n",
      "Loss on last epoch = -1.1326, new lr = 0.01227, global_step = 71494\n",
      "Starting epoch... 167\n",
      "Loss during batch 0 last BPTT = -1.306, lr = 0.01226\n",
      "Loss on last epoch = -1.3047, new lr = 0.01220, global_step = 71939\n",
      "Starting epoch... 168\n",
      "Loss during batch 0 last BPTT = -1.364, lr = 0.01220\n",
      "Loss on last epoch = -1.3332, new lr = 0.01214, global_step = 72364\n",
      "Starting epoch... 169\n",
      "Loss during batch 0 last BPTT = -1.238, lr = 0.01213\n",
      "Loss on last epoch = -1.2232, new lr = 0.01208, global_step = 72786\n",
      "Starting epoch... 170\n",
      "Loss during batch 0 last BPTT = -1.098, lr = 0.01207\n",
      "Loss on last epoch = -1.1744, new lr = 0.01202, global_step = 73202\n",
      "Starting epoch... 171\n",
      "Loss during batch 0 last BPTT = -1.133, lr = 0.01201\n",
      "Loss on last epoch = -1.1803, new lr = 0.01196, global_step = 73634\n",
      "Starting epoch... 172\n",
      "Loss during batch 0 last BPTT = -1.138, lr = 0.01195\n",
      "Loss on last epoch = -1.1556, new lr = 0.01189, global_step = 74073\n",
      "Starting epoch... 173\n",
      "Loss during batch 0 last BPTT = -1.179, lr = 0.01189\n",
      "Loss on last epoch = -1.1026, new lr = 0.01183, global_step = 74517\n",
      "Starting epoch... 174\n",
      "Loss during batch 0 last BPTT = -1.180, lr = 0.01183\n",
      "Loss on last epoch = -1.1379, new lr = 0.01177, global_step = 74944\n",
      "Starting epoch... 175\n",
      "Loss during batch 0 last BPTT = -1.156, lr = 0.01177\n",
      "Loss on last epoch = -1.1473, new lr = 0.01171, global_step = 75391\n",
      "Starting epoch... 176\n",
      "Loss during batch 0 last BPTT = -1.095, lr = 0.01170\n",
      "Loss on last epoch = -1.1271, new lr = 0.01165, global_step = 75823\n",
      "Starting epoch... 177\n",
      "Loss during batch 0 last BPTT = -0.985, lr = 0.01165\n",
      "Loss on last epoch = -1.1279, new lr = 0.01159, global_step = 76267\n",
      "Starting epoch... 178\n",
      "Loss during batch 0 last BPTT = -1.160, lr = 0.01159\n",
      "Loss on last epoch = -1.1472, new lr = 0.01153, global_step = 76710\n",
      "Starting epoch... 179\n",
      "Loss during batch 0 last BPTT = -1.212, lr = 0.01153\n",
      "Loss on last epoch = -1.2062, new lr = 0.01148, global_step = 77125\n",
      "Starting epoch... 180\n",
      "Loss during batch 0 last BPTT = -1.111, lr = 0.01147\n",
      "Loss on last epoch = -1.1584, new lr = 0.01142, global_step = 77561\n",
      "Starting epoch... 181\n",
      "Loss during batch 0 last BPTT = -1.057, lr = 0.01142\n",
      "Loss on last epoch = -1.2260, new lr = 0.01137, global_step = 77968\n",
      "Starting epoch... 182\n",
      "Loss during batch 0 last BPTT = -1.036, lr = 0.01136\n",
      "Loss on last epoch = -1.1946, new lr = 0.01131, global_step = 78405\n",
      "Starting epoch... 183\n",
      "Loss during batch 0 last BPTT = -1.036, lr = 0.01131\n",
      "Loss on last epoch = -1.1319, new lr = 0.01126, global_step = 78837\n",
      "Starting epoch... 184\n",
      "Loss during batch 0 last BPTT = -1.154, lr = 0.01125\n",
      "Loss on last epoch = -1.2118, new lr = 0.01120, global_step = 79273\n",
      "Starting epoch... 185\n",
      "Loss during batch 0 last BPTT = -1.163, lr = 0.01120\n",
      "Loss on last epoch = -1.2358, new lr = 0.01116, global_step = 79639\n",
      "Starting epoch... 186\n",
      "Loss during batch 0 last BPTT = -1.280, lr = 0.01115\n",
      "Loss on last epoch = -1.2069, new lr = 0.01110, global_step = 80064\n",
      "Starting epoch... 187\n",
      "Loss during batch 0 last BPTT = -1.162, lr = 0.01110\n",
      "Loss on last epoch = -1.1809, new lr = 0.01105, global_step = 80465\n",
      "Starting epoch... 188\n",
      "Loss during batch 0 last BPTT = -1.254, lr = 0.01105\n",
      "Loss on last epoch = -1.2723, new lr = 0.01100, global_step = 80901\n",
      "Starting epoch... 189\n",
      "Loss during batch 0 last BPTT = -1.231, lr = 0.01100\n",
      "Loss on last epoch = -1.2335, new lr = 0.01095, global_step = 81318\n",
      "Starting epoch... 190\n",
      "Loss during batch 0 last BPTT = -1.145, lr = 0.01094\n",
      "Loss on last epoch = -1.1680, new lr = 0.01090, global_step = 81764\n",
      "Starting epoch... 191\n",
      "Loss during batch 0 last BPTT = -1.276, lr = 0.01089\n",
      "Loss on last epoch = -1.2215, new lr = 0.01085, global_step = 82188\n",
      "Starting epoch... 192\n",
      "Loss during batch 0 last BPTT = -1.094, lr = 0.01084\n",
      "Loss on last epoch = -1.2256, new lr = 0.01080, global_step = 82602\n",
      "Starting epoch... 193\n",
      "Loss during batch 0 last BPTT = -1.161, lr = 0.01080\n",
      "Loss on last epoch = -1.2285, new lr = 0.01075, global_step = 83042\n",
      "Starting epoch... 194\n",
      "Loss during batch 0 last BPTT = -1.179, lr = 0.01074\n",
      "Loss on last epoch = -1.2108, new lr = 0.01070, global_step = 83480\n",
      "Starting epoch... 195\n",
      "Loss during batch 0 last BPTT = -0.980, lr = 0.01069\n",
      "Loss on last epoch = -1.1938, new lr = 0.01065, global_step = 83894\n",
      "Starting epoch... 196\n",
      "Loss during batch 0 last BPTT = -1.241, lr = 0.01065\n",
      "Loss on last epoch = -1.2092, new lr = 0.01060, global_step = 84313\n",
      "Starting epoch... 197\n",
      "Loss during batch 0 last BPTT = -1.226, lr = 0.01060\n",
      "Loss on last epoch = -1.2679, new lr = 0.01056, global_step = 84734\n",
      "Starting epoch... 198\n",
      "Loss during batch 0 last BPTT = -1.076, lr = 0.01055\n",
      "Loss on last epoch = -1.2206, new lr = 0.01051, global_step = 85157\n",
      "Starting epoch... 199\n",
      "Loss during batch 0 last BPTT = -1.218, lr = 0.01050\n",
      "Loss on last epoch = -1.2571, new lr = 0.01046, global_step = 85594\n",
      "Starting epoch... 200\n",
      "Loss during batch 0 last BPTT = -1.161, lr = 0.01045\n",
      "Loss on last epoch = -1.2026, new lr = 0.01041, global_step = 86016\n",
      "Model saved at ./save.rmtpp/model.ckpt\n",
      "Running evaluation on training data: ...\n",
      "** MAE = 0.018; valid = 43, ACC = 0.209\n",
      "CPU times: user 34min 16s, sys: 3min 43s, total: 38min\n",
      "Wall time: 15min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rmtpp_mdl.train(training_data=data, restart=True, \n",
    "                with_summaries=True, num_epochs=100, with_evals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring from ./save.rmtpp/model.ckpt-86016\n",
      "INFO:tensorflow:Restoring parameters from ./save.rmtpp/model.ckpt-86016\n",
      "Starting epoch... 201\n",
      "Loss during batch 0 last BPTT = -1.222, lr = 0.01041\n",
      "Loss on last epoch = -1.2847, new lr = 0.01037, global_step = 86434\n",
      "Starting epoch... 202\n",
      "Loss during batch 0 last BPTT = -1.313, lr = 0.01037\n",
      "Loss on last epoch = -1.3056, new lr = 0.01032, global_step = 86858\n",
      "Starting epoch... 203\n",
      "Loss during batch 0 last BPTT = -1.187, lr = 0.01032\n",
      "Loss on last epoch = -1.1868, new lr = 0.01028, global_step = 87284\n",
      "Starting epoch... 204\n",
      "Loss during batch 0 last BPTT = -1.095, lr = 0.01028\n",
      "Loss on last epoch = -1.2225, new lr = 0.01023, global_step = 87716\n",
      "Starting epoch... 205\n",
      "Loss during batch 0 last BPTT = -1.180, lr = 0.01023\n",
      "Loss on last epoch = -1.2474, new lr = 0.01019, global_step = 88152\n",
      "Starting epoch... 206\n",
      "Loss during batch 0 last BPTT = -1.094, lr = 0.01019\n",
      "Loss on last epoch = -1.2341, new lr = 0.01015, global_step = 88570\n",
      "Starting epoch... 207\n",
      "Loss during batch 0 last BPTT = -1.216, lr = 0.01014\n",
      "Loss on last epoch = -1.2477, new lr = 0.01010, global_step = 88986\n",
      "Starting epoch... 208\n",
      "Loss during batch 0 last BPTT = -1.180, lr = 0.01010\n",
      "Loss on last epoch = -1.2524, new lr = 0.01006, global_step = 89405\n",
      "Starting epoch... 209\n",
      "Loss during batch 0 last BPTT = -1.262, lr = 0.01006\n",
      "Loss on last epoch = -1.2986, new lr = 0.01002, global_step = 89819\n",
      "Starting epoch... 210\n",
      "Loss during batch 0 last BPTT = -1.142, lr = 0.01002\n",
      "Loss on last epoch = -1.2325, new lr = 0.00997, global_step = 90266\n",
      "Starting epoch... 211\n",
      "Loss during batch 0 last BPTT = -1.293, lr = 0.00997\n",
      "Loss on last epoch = -1.2407, new lr = 0.00993, global_step = 90691\n",
      "Starting epoch... 212\n",
      "Loss during batch 0 last BPTT = -1.283, lr = 0.00993\n",
      "Loss on last epoch = -1.2850, new lr = 0.00989, global_step = 91121\n",
      "Starting epoch... 213\n",
      "Loss during batch 0 last BPTT = -1.328, lr = 0.00988\n",
      "Loss on last epoch = -1.3015, new lr = 0.00985, global_step = 91552\n",
      "Starting epoch... 214\n",
      "Loss during batch 0 last BPTT = -1.201, lr = 0.00984\n",
      "Loss on last epoch = -1.2188, new lr = 0.00981, global_step = 91986\n",
      "Starting epoch... 215\n",
      "Loss during batch 0 last BPTT = -1.250, lr = 0.00980\n",
      "Loss on last epoch = -1.2626, new lr = 0.00976, global_step = 92422\n",
      "Starting epoch... 216\n",
      "Loss during batch 0 last BPTT = -1.365, lr = 0.00976\n",
      "Loss on last epoch = -1.2880, new lr = 0.00972, global_step = 92865\n",
      "Starting epoch... 217\n",
      "Loss during batch 0 last BPTT = -1.066, lr = 0.00972\n",
      "Loss on last epoch = -1.2613, new lr = 0.00968, global_step = 93289\n",
      "Starting epoch... 218\n",
      "Loss during batch 0 last BPTT = -1.213, lr = 0.00968\n",
      "Loss on last epoch = -1.2612, new lr = 0.00964, global_step = 93697\n",
      "Starting epoch... 219\n",
      "Loss during batch 0 last BPTT = -1.103, lr = 0.00964\n",
      "Loss on last epoch = -1.3041, new lr = 0.00960, global_step = 94128\n",
      "Starting epoch... 220\n",
      "Loss during batch 0 last BPTT = -1.136, lr = 0.00960\n",
      "Loss on last epoch = -1.2494, new lr = 0.00956, global_step = 94566\n",
      "Starting epoch... 221\n",
      "Loss during batch 0 last BPTT = -1.122, lr = 0.00956\n",
      "Loss on last epoch = -1.2513, new lr = 0.00952, global_step = 94996\n",
      "Starting epoch... 222\n",
      "Loss during batch 0 last BPTT = -1.209, lr = 0.00952\n",
      "Loss on last epoch = -1.2949, new lr = 0.00948, global_step = 95430\n",
      "Starting epoch... 223\n",
      "Loss during batch 0 last BPTT = -1.204, lr = 0.00948\n",
      "Loss on last epoch = -1.2553, new lr = 0.00945, global_step = 95854\n",
      "Starting epoch... 224\n",
      "Loss during batch 0 last BPTT = -1.295, lr = 0.00944\n",
      "Loss on last epoch = -1.3334, new lr = 0.00941, global_step = 96264\n",
      "Starting epoch... 225\n",
      "Loss during batch 0 last BPTT = -1.265, lr = 0.00941\n",
      "Loss on last epoch = -1.3197, new lr = 0.00937, global_step = 96679\n",
      "Starting epoch... 226\n",
      "Loss during batch 0 last BPTT = -1.219, lr = 0.00937\n",
      "Loss on last epoch = -1.3227, new lr = 0.00934, global_step = 97100\n",
      "Starting epoch... 227\n",
      "Loss during batch 0 last BPTT = -1.333, lr = 0.00933\n",
      "Loss on last epoch = -1.2635, new lr = 0.00930, global_step = 97542\n",
      "Starting epoch... 228\n",
      "Loss during batch 0 last BPTT = -1.256, lr = 0.00929\n",
      "Loss on last epoch = -1.2647, new lr = 0.00926, global_step = 97979\n",
      "Starting epoch... 229\n",
      "Loss during batch 0 last BPTT = -1.195, lr = 0.00926\n",
      "Loss on last epoch = -1.3099, new lr = 0.00923, global_step = 98397\n",
      "Starting epoch... 230\n",
      "Loss during batch 0 last BPTT = -1.210, lr = 0.00922\n",
      "Loss on last epoch = -1.2708, new lr = 0.00919, global_step = 98830\n",
      "Starting epoch... 231\n",
      "Loss during batch 0 last BPTT = -1.247, lr = 0.00918\n",
      "Loss on last epoch = -1.3653, new lr = 0.00915, global_step = 99243\n",
      "Starting epoch... 232\n",
      "Loss during batch 0 last BPTT = -1.329, lr = 0.00915\n",
      "Loss on last epoch = -1.3684, new lr = 0.00912, global_step = 99659\n",
      "Starting epoch... 233\n",
      "Loss during batch 0 last BPTT = -1.327, lr = 0.00911\n",
      "Loss on last epoch = -1.3129, new lr = 0.00908, global_step = 100072\n",
      "Starting epoch... 234\n",
      "Loss during batch 0 last BPTT = -1.312, lr = 0.00908\n",
      "Loss on last epoch = -1.2605, new lr = 0.00905, global_step = 100499\n",
      "Starting epoch... 235\n",
      "Loss during batch 0 last BPTT = -1.185, lr = 0.00904\n",
      "Loss on last epoch = -1.1526, new lr = 0.00902, global_step = 100923\n",
      "Starting epoch... 236\n",
      "Loss during batch 0 last BPTT = -1.203, lr = 0.00901\n",
      "Loss on last epoch = -1.2134, new lr = 0.00898, global_step = 101345\n",
      "Starting epoch... 237\n",
      "Loss during batch 0 last BPTT = -1.210, lr = 0.00898\n",
      "Loss on last epoch = -1.2956, new lr = 0.00895, global_step = 101767\n",
      "Starting epoch... 238\n",
      "Loss during batch 0 last BPTT = -1.318, lr = 0.00894\n",
      "Loss on last epoch = -1.2494, new lr = 0.00891, global_step = 102198\n",
      "Starting epoch... 239\n",
      "Loss during batch 0 last BPTT = -1.251, lr = 0.00891\n",
      "Loss on last epoch = -1.2965, new lr = 0.00888, global_step = 102585\n",
      "Starting epoch... 240\n",
      "Loss during batch 0 last BPTT = -1.336, lr = 0.00888\n",
      "Loss on last epoch = -1.2937, new lr = 0.00885, global_step = 103005\n",
      "Starting epoch... 241\n",
      "Loss during batch 0 last BPTT = -1.152, lr = 0.00885\n",
      "Loss on last epoch = -1.3010, new lr = 0.00882, global_step = 103419\n",
      "Starting epoch... 242\n",
      "Loss during batch 0 last BPTT = -1.208, lr = 0.00881\n",
      "Loss on last epoch = -1.2900, new lr = 0.00878, global_step = 103851\n",
      "Starting epoch... 243\n",
      "Loss during batch 0 last BPTT = -1.169, lr = 0.00878\n",
      "Loss on last epoch = -1.2325, new lr = 0.00875, global_step = 104269\n",
      "Starting epoch... 244\n",
      "Loss during batch 0 last BPTT = -1.329, lr = 0.00875\n",
      "Loss on last epoch = -1.3739, new lr = 0.00872, global_step = 104699\n",
      "Starting epoch... 245\n",
      "Loss during batch 0 last BPTT = -1.276, lr = 0.00872\n",
      "Loss on last epoch = -1.1791, new lr = 0.00868, global_step = 105146\n",
      "Starting epoch... 246\n",
      "Loss during batch 0 last BPTT = -1.152, lr = 0.00868\n",
      "Loss on last epoch = -1.2474, new lr = 0.00865, global_step = 105588\n",
      "Starting epoch... 247\n",
      "Loss during batch 0 last BPTT = -1.247, lr = 0.00865\n",
      "Loss on last epoch = -1.3035, new lr = 0.00862, global_step = 106016\n",
      "Starting epoch... 248\n",
      "Loss during batch 0 last BPTT = -1.275, lr = 0.00862\n",
      "Loss on last epoch = -1.2710, new lr = 0.00859, global_step = 106447\n",
      "Starting epoch... 249\n",
      "Loss during batch 0 last BPTT = -1.247, lr = 0.00858\n",
      "Loss on last epoch = -1.2634, new lr = 0.00856, global_step = 106847\n",
      "Starting epoch... 250\n",
      "Loss during batch 0 last BPTT = -1.273, lr = 0.00856\n",
      "Loss on last epoch = -1.3291, new lr = 0.00853, global_step = 107264\n",
      "Starting epoch... 251\n",
      "Loss during batch 0 last BPTT = -1.301, lr = 0.00852\n",
      "Loss on last epoch = -1.3357, new lr = 0.00850, global_step = 107675\n",
      "Starting epoch... 252\n",
      "Loss during batch 0 last BPTT = -1.454, lr = 0.00849\n",
      "Loss on last epoch = -1.3081, new lr = 0.00847, global_step = 108096\n",
      "Starting epoch... 253\n",
      "Loss during batch 0 last BPTT = -1.166, lr = 0.00846\n",
      "Loss on last epoch = -1.2862, new lr = 0.00844, global_step = 108544\n",
      "Starting epoch... 254\n",
      "Loss during batch 0 last BPTT = -1.226, lr = 0.00843\n",
      "Loss on last epoch = -1.3084, new lr = 0.00841, global_step = 108947\n",
      "Starting epoch... 255\n",
      "Loss during batch 0 last BPTT = -1.294, lr = 0.00840\n",
      "Loss on last epoch = -1.3001, new lr = 0.00838, global_step = 109371\n",
      "Starting epoch... 256\n",
      "Loss during batch 0 last BPTT = -1.272, lr = 0.00838\n",
      "Loss on last epoch = -1.3158, new lr = 0.00835, global_step = 109794\n",
      "Starting epoch... 257\n",
      "Loss during batch 0 last BPTT = -1.234, lr = 0.00835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on last epoch = -1.2903, new lr = 0.00832, global_step = 110218\n",
      "Starting epoch... 258\n",
      "Loss during batch 0 last BPTT = -1.268, lr = 0.00832\n",
      "Loss on last epoch = -1.2982, new lr = 0.00829, global_step = 110631\n",
      "Starting epoch... 259\n",
      "Loss during batch 0 last BPTT = -1.311, lr = 0.00829\n",
      "Loss on last epoch = -1.3290, new lr = 0.00826, global_step = 111055\n",
      "Starting epoch... 260\n",
      "Loss during batch 0 last BPTT = -1.186, lr = 0.00826\n",
      "Loss on last epoch = -1.3373, new lr = 0.00823, global_step = 111494\n",
      "Starting epoch... 261\n",
      "Loss during batch 0 last BPTT = -1.386, lr = 0.00823\n",
      "Loss on last epoch = -1.3333, new lr = 0.00820, global_step = 111937\n",
      "Starting epoch... 262\n",
      "Loss during batch 0 last BPTT = -1.141, lr = 0.00820\n",
      "Loss on last epoch = -1.2447, new lr = 0.00817, global_step = 112364\n",
      "Starting epoch... 263\n",
      "Loss during batch 0 last BPTT = -1.240, lr = 0.00817\n",
      "Loss on last epoch = -1.2965, new lr = 0.00814, global_step = 112819\n",
      "Starting epoch... 264\n",
      "Loss during batch 0 last BPTT = -1.283, lr = 0.00814\n",
      "Loss on last epoch = -1.2027, new lr = 0.00811, global_step = 113259\n",
      "Starting epoch... 265\n",
      "Loss during batch 0 last BPTT = -1.029, lr = 0.00811\n",
      "Loss on last epoch = -1.1593, new lr = 0.00808, global_step = 113688\n",
      "Starting epoch... 266\n",
      "Loss during batch 0 last BPTT = -1.172, lr = 0.00808\n",
      "Loss on last epoch = -1.2567, new lr = 0.00806, global_step = 114122\n",
      "Starting epoch... 267\n",
      "Loss during batch 0 last BPTT = -1.233, lr = 0.00805\n",
      "Loss on last epoch = -1.2659, new lr = 0.00803, global_step = 114530\n",
      "Starting epoch... 268\n",
      "Loss during batch 0 last BPTT = -1.361, lr = 0.00803\n",
      "Loss on last epoch = -1.3518, new lr = 0.00800, global_step = 114972\n",
      "Starting epoch... 269\n",
      "Loss during batch 0 last BPTT = -1.318, lr = 0.00800\n",
      "Loss on last epoch = -1.2806, new lr = 0.00797, global_step = 115410\n",
      "Starting epoch... 270\n",
      "Loss during batch 0 last BPTT = -1.233, lr = 0.00797\n",
      "Loss on last epoch = -1.2489, new lr = 0.00795, global_step = 115855\n",
      "Starting epoch... 271\n",
      "Loss during batch 0 last BPTT = -1.254, lr = 0.00794\n",
      "Loss on last epoch = -1.2586, new lr = 0.00792, global_step = 116279\n",
      "Starting epoch... 272\n",
      "Loss during batch 0 last BPTT = -1.360, lr = 0.00792\n",
      "Loss on last epoch = -1.3266, new lr = 0.00789, global_step = 116697\n",
      "Starting epoch... 273\n",
      "Loss during batch 0 last BPTT = -1.145, lr = 0.00789\n",
      "Loss on last epoch = -1.2690, new lr = 0.00787, global_step = 117105\n",
      "Starting epoch... 274\n",
      "Loss during batch 0 last BPTT = -1.259, lr = 0.00787\n",
      "Loss on last epoch = -1.3326, new lr = 0.00784, global_step = 117522\n",
      "Starting epoch... 275\n",
      "Loss during batch 0 last BPTT = -1.284, lr = 0.00784\n",
      "Loss on last epoch = -1.2611, new lr = 0.00782, global_step = 117927\n",
      "Starting epoch... 276\n",
      "Loss during batch 0 last BPTT = -1.256, lr = 0.00781\n",
      "Loss on last epoch = -1.3879, new lr = 0.00779, global_step = 118372\n",
      "Starting epoch... 277\n",
      "Loss during batch 0 last BPTT = -1.434, lr = 0.00779\n",
      "Loss on last epoch = -1.4023, new lr = 0.00776, global_step = 118790\n",
      "Starting epoch... 278\n",
      "Loss during batch 0 last BPTT = -1.352, lr = 0.00776\n",
      "Loss on last epoch = -1.3203, new lr = 0.00774, global_step = 119210\n",
      "Starting epoch... 279\n",
      "Loss during batch 0 last BPTT = -1.227, lr = 0.00774\n",
      "Loss on last epoch = -1.3731, new lr = 0.00771, global_step = 119630\n",
      "Starting epoch... 280\n",
      "Loss during batch 0 last BPTT = -1.364, lr = 0.00771\n",
      "Loss on last epoch = -1.3603, new lr = 0.00769, global_step = 120079\n",
      "Starting epoch... 281\n",
      "Loss during batch 0 last BPTT = -1.183, lr = 0.00769\n",
      "Loss on last epoch = -1.3035, new lr = 0.00766, global_step = 120503\n",
      "Starting epoch... 282\n",
      "Loss during batch 0 last BPTT = -1.330, lr = 0.00766\n",
      "Loss on last epoch = -1.3720, new lr = 0.00764, global_step = 120938\n",
      "Starting epoch... 283\n",
      "Loss during batch 0 last BPTT = -1.263, lr = 0.00763\n",
      "Loss on last epoch = -1.2902, new lr = 0.00761, global_step = 121360\n",
      "Starting epoch... 284\n",
      "Loss during batch 0 last BPTT = -1.419, lr = 0.00761\n",
      "Loss on last epoch = -1.4076, new lr = 0.00759, global_step = 121783\n",
      "Starting epoch... 285\n",
      "Loss during batch 0 last BPTT = -1.324, lr = 0.00759\n",
      "Loss on last epoch = -1.3226, new lr = 0.00756, global_step = 122218\n",
      "Starting epoch... 286\n",
      "Loss during batch 0 last BPTT = -1.197, lr = 0.00756\n",
      "Loss on last epoch = -1.3086, new lr = 0.00754, global_step = 122652\n",
      "Starting epoch... 287\n",
      "Loss during batch 0 last BPTT = -1.329, lr = 0.00754\n",
      "Loss on last epoch = -1.3723, new lr = 0.00751, global_step = 123101\n",
      "Starting epoch... 288\n",
      "Loss during batch 0 last BPTT = -1.377, lr = 0.00751\n",
      "Loss on last epoch = -1.3565, new lr = 0.00749, global_step = 123518\n",
      "Starting epoch... 289\n",
      "Loss during batch 0 last BPTT = -1.364, lr = 0.00749\n",
      "Loss on last epoch = -1.3293, new lr = 0.00746, global_step = 123960\n",
      "Starting epoch... 290\n",
      "Loss during batch 0 last BPTT = -1.246, lr = 0.00746\n",
      "Loss on last epoch = -1.3342, new lr = 0.00744, global_step = 124356\n",
      "Starting epoch... 291\n",
      "Loss during batch 0 last BPTT = -1.322, lr = 0.00744\n",
      "Loss on last epoch = -1.4165, new lr = 0.00742, global_step = 124772\n",
      "Starting epoch... 292\n",
      "Loss during batch 0 last BPTT = -1.388, lr = 0.00742\n",
      "Loss on last epoch = -1.3454, new lr = 0.00740, global_step = 125200\n",
      "Starting epoch... 293\n",
      "Loss during batch 0 last BPTT = -1.266, lr = 0.00739\n",
      "Loss on last epoch = -1.3205, new lr = 0.00737, global_step = 125631\n",
      "Starting epoch... 294\n",
      "Loss during batch 0 last BPTT = -1.330, lr = 0.00737\n",
      "Loss on last epoch = -1.3994, new lr = 0.00735, global_step = 126050\n",
      "Starting epoch... 295\n",
      "Loss during batch 0 last BPTT = -1.319, lr = 0.00735\n",
      "Loss on last epoch = -1.3221, new lr = 0.00733, global_step = 126499\n",
      "Starting epoch... 296\n",
      "Loss during batch 0 last BPTT = -1.286, lr = 0.00732\n",
      "Loss on last epoch = -1.3741, new lr = 0.00730, global_step = 126905\n",
      "Starting epoch... 297\n",
      "Loss during batch 0 last BPTT = -1.416, lr = 0.00730\n",
      "Loss on last epoch = -1.4220, new lr = 0.00728, global_step = 127328\n",
      "Starting epoch... 298\n",
      "Loss during batch 0 last BPTT = -1.257, lr = 0.00728\n",
      "Loss on last epoch = -1.3062, new lr = 0.00726, global_step = 127738\n",
      "Starting epoch... 299\n",
      "Loss during batch 0 last BPTT = -1.355, lr = 0.00726\n",
      "Loss on last epoch = -1.3927, new lr = 0.00724, global_step = 128180\n",
      "Starting epoch... 300\n",
      "Loss during batch 0 last BPTT = -1.277, lr = 0.00724\n",
      "Loss on last epoch = -1.3587, new lr = 0.00721, global_step = 128606\n",
      "Model saved at ./save.rmtpp/model.ckpt\n",
      "Running evaluation on training data: ...\n",
      "** MAE = 0.017; valid = 43, ACC = 0.209\n",
      "CPU times: user 34min 12s, sys: 3min 43s, total: 37min 56s\n",
      "Wall time: 15min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rmtpp_mdl.train(training_data=data, restart=True, \n",
    "                with_summaries=True, num_epochs=100, with_evals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch... 0\n",
      "Loss during batch 0 last BPTT = -0.420, lr = 0.09965\n",
      "Loss during batch 10 last BPTT = -1.412, lr = 0.09629\n",
      "Loss during batch 20 last BPTT = -1.343, lr = 0.09315\n",
      "Loss during batch 30 last BPTT = -1.428, lr = 0.09021\n",
      "Loss during batch 40 last BPTT = -1.548, lr = 0.08745\n",
      "Loss during batch 50 last BPTT = -1.583, lr = 0.08485\n",
      "Loss during batch 60 last BPTT = -1.455, lr = 0.08241\n",
      "Loss during batch 70 last BPTT = -1.252, lr = 0.08010\n",
      "Loss during batch 80 last BPTT = -1.474, lr = 0.07791\n",
      "Loss on last epoch = -1.3616, new lr = 0.07770, global_step = 2870\n",
      "Model saved at ./save.rmtpp/model.ckpt\n",
      "CPU times: user 3min 47s, sys: 30 s, total: 4min 17s\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rmtpp_mdl.train(training_data=data, restart=False, \n",
    "                with_summaries=True, num_epochs=1, with_evals=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch... 0\n",
      "Loss during batch 0 last BPTT = 1.179, lr = 0.09965\n",
      "Loss during batch 10 last BPTT = -1.354, lr = 0.09629\n",
      "Loss during batch 20 last BPTT = -1.547, lr = 0.09315\n",
      "Loss during batch 30 last BPTT = -1.476, lr = 0.09021\n",
      "Loss during batch 40 last BPTT = -1.418, lr = 0.08745\n",
      "Loss during batch 50 last BPTT = -1.591, lr = 0.08485\n",
      "Loss during batch 60 last BPTT = -1.307, lr = 0.08241\n",
      "Loss during batch 70 last BPTT = -1.508, lr = 0.08010\n",
      "Loss during batch 80 last BPTT = -1.214, lr = 0.07791\n",
      "Loss on last epoch = -1.3422, new lr = 0.07770, global_step = 2870\n",
      "Model saved at ./save.rmtpp/model.ckpt\n",
      "Running evaluation on training data: ...\n",
      "** MAE = 0.010; valid = 377855, ACC = 0.435\n",
      "CPU times: user 5min 1s, sys: 35.6 s, total: 5min 36s\n",
      "Wall time: 10min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rmtpp_mdl.train(training_data=data, restart=False, \n",
    "                with_summaries=True, num_epochs=1, with_evals=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Wem = tf_rmtpp.def_opts.Wem(data['num_categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run -i ../tf_rmtpp/tests/test_sample.py\n",
    "f_d = rmtpp_mdl.make_feed_dict(data, [1, 2], 0)\n",
    "np_batch = map_feed_dict_to_np(f_d, rmtpp_mdl=rmtpp_mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np_vals = numpy_LL(np_batch, \n",
    "                   num_categories=data['num_categories'], \n",
    "                   bt=np.log(base_rate),\n",
    "                   _opts=tf_rmtpp.def_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_vals = rmtpp_mdl.sess.run(\n",
    "    {\n",
    "        'loss': rmtpp_mdl.loss,\n",
    "        'mark_LLs': rmtpp_mdl.mark_LLs,\n",
    "        'time_LLs': rmtpp_mdl.time_LLs,\n",
    "        'hidden_states': rmtpp_mdl.hidden_states,\n",
    "        'log_lambdas': rmtpp_mdl.log_lambdas,\n",
    "        'event_preds': rmtpp_mdl.event_preds\n",
    "    }, \n",
    "    feed_dict=f_d\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.090437554471984233, -0.090437494)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_vals['loss'], tf_vals['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# key = 'mark_LLs'\n",
    "# key = 'hidden_states'\n",
    "key = 'event_preds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmtpp_mdl.sess.run(rmtpp_mdl.bk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.50000003,  0.50000003,  0.50000003,  0.50000003,  0.50000003,\n",
       "          0.50000003,  0.50000003,  0.50000003,  0.50000003,  0.50000003,\n",
       "          0.50000003,  0.50000003,  0.50000003,  0.50000003,  0.50000003,\n",
       "          0.50000003,  0.50000003,  0.50000003,  0.50000003,  0.50000003,\n",
       "          0.50000003,  0.50000003],\n",
       "        [ 0.49999997,  0.49999997,  0.49999997,  0.49999997,  0.49999997,\n",
       "          0.49999997,  0.49999997,  0.49999997,  0.49999997,  0.49999997,\n",
       "          0.49999997,  0.49999997,  0.49999997,  0.49999997,  0.49999997,\n",
       "          0.49999997,  0.49999997,  0.49999997,  0.49999997,  0.49999997,\n",
       "          0.49999997,  0.49999997]]),\n",
       " array([[ 0.50000007,  0.50000007,  0.50000007,  0.50000007,  0.50000007,\n",
       "          0.50000007,  0.50000007,  0.50000007,  0.50000007,  0.50000007,\n",
       "          0.50000007,  0.50000007,  0.50000007,  0.50000007,  0.50000007,\n",
       "          0.50000007,  0.50000007,  0.50000007,  0.50000007,  0.50000007,\n",
       "          0.50000007,  0.50000007],\n",
       "        [ 0.49999993,  0.49999993,  0.49999993,  0.49999993,  0.49999993,\n",
       "          0.49999993,  0.49999993,  0.49999993,  0.49999993,  0.49999993,\n",
       "          0.49999993,  0.49999993,  0.49999993,  0.49999993,  0.49999993,\n",
       "          0.49999993,  0.49999993,  0.49999993,  0.49999993,  0.49999993,\n",
       "          0.49999993,  0.49999993]]),\n",
       " array([[ 0.50000012,  0.50000012,  0.50000012,  0.50000012,  0.50000012,\n",
       "          0.50000012,  0.50000012,  0.50000012,  0.50000012,  0.50000012,\n",
       "          0.50000012,  0.50000012,  0.50000012,  0.50000012,  0.50000012,\n",
       "          0.50000012,  0.50000012,  0.50000012,  0.50000012,  0.50000012,\n",
       "          0.50000012,  0.50000012],\n",
       "        [ 0.49999988,  0.49999988,  0.49999988,  0.49999988,  0.49999988,\n",
       "          0.49999988,  0.49999988,  0.49999988,  0.49999988,  0.49999988,\n",
       "          0.49999988,  0.49999988,  0.49999988,  0.49999988,  0.49999988,\n",
       "          0.49999988,  0.49999988,  0.49999988,  0.49999988,  0.49999988,\n",
       "          0.49999988,  0.49999988]]),\n",
       " array([[ 0.49999999,  0.49999999,  0.49999999,  0.49999999,  0.49999999,\n",
       "          0.49999999,  0.49999999,  0.49999999,  0.49999999,  0.49999999,\n",
       "          0.49999999,  0.49999999,  0.49999999,  0.49999999,  0.49999999,\n",
       "          0.49999999,  0.49999999,  0.49999999,  0.49999999,  0.49999999,\n",
       "          0.49999999,  0.49999999],\n",
       "        [ 0.50000001,  0.50000001,  0.50000001,  0.50000001,  0.50000001,\n",
       "          0.50000001,  0.50000001,  0.50000001,  0.50000001,  0.50000001,\n",
       "          0.50000001,  0.50000001,  0.50000001,  0.50000001,  0.50000001,\n",
       "          0.50000001,  0.50000001,  0.50000001,  0.50000001,  0.50000001,\n",
       "          0.50000001,  0.50000001]]),\n",
       " array([[ 0.50000005,  0.50000005,  0.50000005,  0.50000005,  0.50000005,\n",
       "          0.50000005,  0.50000005,  0.50000005,  0.50000005,  0.50000005,\n",
       "          0.50000005,  0.50000005,  0.50000005,  0.50000005,  0.50000005,\n",
       "          0.50000005,  0.50000005,  0.50000005,  0.50000005,  0.50000005,\n",
       "          0.50000005,  0.50000005],\n",
       "        [ 0.49999995,  0.49999995,  0.49999995,  0.49999995,  0.49999995,\n",
       "          0.49999995,  0.49999995,  0.49999995,  0.49999995,  0.49999995,\n",
       "          0.49999995,  0.49999995,  0.49999995,  0.49999995,  0.49999995,\n",
       "          0.49999995,  0.49999995,  0.49999995,  0.49999995,  0.49999995,\n",
       "          0.49999995,  0.49999995]]),\n",
       " array([[ 0.50000005,  0.50000005,  0.50000005,  0.50000005,  0.50000005,\n",
       "          0.50000005,  0.50000005,  0.50000005,  0.50000005,  0.50000005,\n",
       "          0.50000005,  0.50000005,  0.50000005,  0.50000005,  0.50000005,\n",
       "          0.50000005,  0.50000005,  0.50000005,  0.50000005,  0.50000005,\n",
       "          0.50000005,  0.50000005],\n",
       "        [ 0.49999995,  0.49999995,  0.49999995,  0.49999995,  0.49999995,\n",
       "          0.49999995,  0.49999995,  0.49999995,  0.49999995,  0.49999995,\n",
       "          0.49999995,  0.49999995,  0.49999995,  0.49999995,  0.49999995,\n",
       "          0.49999995,  0.49999995,  0.49999995,  0.49999995,  0.49999995,\n",
       "          0.49999995,  0.49999995]]),\n",
       " array([[ 0.49999971,  0.49999971,  0.49999971,  0.49999971,  0.49999971,\n",
       "          0.49999971,  0.49999971,  0.49999971,  0.49999971,  0.49999971,\n",
       "          0.49999971,  0.49999971,  0.49999971,  0.49999971,  0.49999971,\n",
       "          0.49999971,  0.49999971,  0.49999971,  0.49999971,  0.49999971,\n",
       "          0.49999971,  0.49999971],\n",
       "        [ 0.50000029,  0.50000029,  0.50000029,  0.50000029,  0.50000029,\n",
       "          0.50000029,  0.50000029,  0.50000029,  0.50000029,  0.50000029,\n",
       "          0.50000029,  0.50000029,  0.50000029,  0.50000029,  0.50000029,\n",
       "          0.50000029,  0.50000029,  0.50000029,  0.50000029,  0.50000029,\n",
       "          0.50000029,  0.50000029]]),\n",
       " array([[ 0.49999975,  0.49999975,  0.49999975,  0.49999975,  0.49999975,\n",
       "          0.49999975,  0.49999975,  0.49999975,  0.49999975,  0.49999975,\n",
       "          0.49999975,  0.49999975,  0.49999975,  0.49999975,  0.49999975,\n",
       "          0.49999975,  0.49999975,  0.49999975,  0.49999975,  0.49999975,\n",
       "          0.49999975,  0.49999975],\n",
       "        [ 0.50000025,  0.50000025,  0.50000025,  0.50000025,  0.50000025,\n",
       "          0.50000025,  0.50000025,  0.50000025,  0.50000025,  0.50000025,\n",
       "          0.50000025,  0.50000025,  0.50000025,  0.50000025,  0.50000025,\n",
       "          0.50000025,  0.50000025,  0.50000025,  0.50000025,  0.50000025,\n",
       "          0.50000025,  0.50000025]]),\n",
       " array([[ 0.49999971,  0.49999971,  0.49999971,  0.49999971,  0.49999971,\n",
       "          0.49999971,  0.49999971,  0.49999971,  0.49999971,  0.49999971,\n",
       "          0.49999971,  0.49999971,  0.49999971,  0.49999971,  0.49999971,\n",
       "          0.49999971,  0.49999971,  0.49999971,  0.49999971,  0.49999971,\n",
       "          0.49999971,  0.49999971],\n",
       "        [ 0.50000029,  0.50000029,  0.50000029,  0.50000029,  0.50000029,\n",
       "          0.50000029,  0.50000029,  0.50000029,  0.50000029,  0.50000029,\n",
       "          0.50000029,  0.50000029,  0.50000029,  0.50000029,  0.50000029,\n",
       "          0.50000029,  0.50000029,  0.50000029,  0.50000029,  0.50000029,\n",
       "          0.50000029,  0.50000029]]),\n",
       " array([[ 0.4999997,  0.4999997,  0.4999997,  0.4999997,  0.4999997,\n",
       "          0.4999997,  0.4999997,  0.4999997,  0.4999997,  0.4999997,\n",
       "          0.4999997,  0.4999997,  0.4999997,  0.4999997,  0.4999997,\n",
       "          0.4999997,  0.4999997,  0.4999997,  0.4999997,  0.4999997,\n",
       "          0.4999997,  0.4999997],\n",
       "        [ 0.5000003,  0.5000003,  0.5000003,  0.5000003,  0.5000003,\n",
       "          0.5000003,  0.5000003,  0.5000003,  0.5000003,  0.5000003,\n",
       "          0.5000003,  0.5000003,  0.5000003,  0.5000003,  0.5000003,\n",
       "          0.5000003,  0.5000003,  0.5000003,  0.5000003,  0.5000003,\n",
       "          0.5000003,  0.5000003]]),\n",
       " array([[ 0.49999978,  0.49999978,  0.49999978,  0.49999978,  0.49999978,\n",
       "          0.49999978,  0.49999978,  0.49999978,  0.49999978,  0.49999978,\n",
       "          0.49999978,  0.49999978,  0.49999978,  0.49999978,  0.49999978,\n",
       "          0.49999978,  0.49999978,  0.49999978,  0.49999978,  0.49999978,\n",
       "          0.49999978,  0.49999978],\n",
       "        [ 0.50000022,  0.50000022,  0.50000022,  0.50000022,  0.50000022,\n",
       "          0.50000022,  0.50000022,  0.50000022,  0.50000022,  0.50000022,\n",
       "          0.50000022,  0.50000022,  0.50000022,  0.50000022,  0.50000022,\n",
       "          0.50000022,  0.50000022,  0.50000022,  0.50000022,  0.50000022,\n",
       "          0.50000022,  0.50000022]]),\n",
       " array([[ 0.4999998,  0.4999998,  0.4999998,  0.4999998,  0.4999998,\n",
       "          0.4999998,  0.4999998,  0.4999998,  0.4999998,  0.4999998,\n",
       "          0.4999998,  0.4999998,  0.4999998,  0.4999998,  0.4999998,\n",
       "          0.4999998,  0.4999998,  0.4999998,  0.4999998,  0.4999998,\n",
       "          0.4999998,  0.4999998],\n",
       "        [ 0.5000002,  0.5000002,  0.5000002,  0.5000002,  0.5000002,\n",
       "          0.5000002,  0.5000002,  0.5000002,  0.5000002,  0.5000002,\n",
       "          0.5000002,  0.5000002,  0.5000002,  0.5000002,  0.5000002,\n",
       "          0.5000002,  0.5000002,  0.5000002,  0.5000002,  0.5000002,\n",
       "          0.5000002,  0.5000002]]),\n",
       " array([[ 0.49999985,  0.49999985,  0.49999985,  0.49999985,  0.49999985,\n",
       "          0.49999985,  0.49999985,  0.49999985,  0.49999985,  0.49999985,\n",
       "          0.49999985,  0.49999985,  0.49999985,  0.49999985,  0.49999985,\n",
       "          0.49999985,  0.49999985,  0.49999985,  0.49999985,  0.49999985,\n",
       "          0.49999985,  0.49999985],\n",
       "        [ 0.50000015,  0.50000015,  0.50000015,  0.50000015,  0.50000015,\n",
       "          0.50000015,  0.50000015,  0.50000015,  0.50000015,  0.50000015,\n",
       "          0.50000015,  0.50000015,  0.50000015,  0.50000015,  0.50000015,\n",
       "          0.50000015,  0.50000015,  0.50000015,  0.50000015,  0.50000015,\n",
       "          0.50000015,  0.50000015]]),\n",
       " array([[ 0.49999993,  0.49999993,  0.49999993,  0.49999993,  0.49999993,\n",
       "          0.49999993,  0.49999993,  0.49999993,  0.49999993,  0.49999993,\n",
       "          0.49999993,  0.49999993,  0.49999993,  0.49999993,  0.49999993,\n",
       "          0.49999993,  0.49999993,  0.49999993,  0.49999993,  0.49999993,\n",
       "          0.49999993,  0.49999993],\n",
       "        [ 0.50000007,  0.50000007,  0.50000007,  0.50000007,  0.50000007,\n",
       "          0.50000007,  0.50000007,  0.50000007,  0.50000007,  0.50000007,\n",
       "          0.50000007,  0.50000007,  0.50000007,  0.50000007,  0.50000007,\n",
       "          0.50000007,  0.50000007,  0.50000007,  0.50000007,  0.50000007,\n",
       "          0.50000007,  0.50000007]]),\n",
       " array([[ 0.49999988,  0.49999988,  0.49999988,  0.49999988,  0.49999988,\n",
       "          0.49999988,  0.49999988,  0.49999988,  0.49999988,  0.49999988,\n",
       "          0.49999988,  0.49999988,  0.49999988,  0.49999988,  0.49999988,\n",
       "          0.49999988,  0.49999988,  0.49999988,  0.49999988,  0.49999988,\n",
       "          0.49999988,  0.49999988],\n",
       "        [ 0.50000012,  0.50000012,  0.50000012,  0.50000012,  0.50000012,\n",
       "          0.50000012,  0.50000012,  0.50000012,  0.50000012,  0.50000012,\n",
       "          0.50000012,  0.50000012,  0.50000012,  0.50000012,  0.50000012,\n",
       "          0.50000012,  0.50000012,  0.50000012,  0.50000012,  0.50000012,\n",
       "          0.50000012,  0.50000012]]),\n",
       " array([[ 0.49999979,  0.49999979,  0.49999979,  0.49999979,  0.49999979,\n",
       "          0.49999979,  0.49999979,  0.49999979,  0.49999979,  0.49999979,\n",
       "          0.49999979,  0.49999979,  0.49999979,  0.49999979,  0.49999979,\n",
       "          0.49999979,  0.49999979,  0.49999979,  0.49999979,  0.49999979,\n",
       "          0.49999979,  0.49999979],\n",
       "        [ 0.50000021,  0.50000021,  0.50000021,  0.50000021,  0.50000021,\n",
       "          0.50000021,  0.50000021,  0.50000021,  0.50000021,  0.50000021,\n",
       "          0.50000021,  0.50000021,  0.50000021,  0.50000021,  0.50000021,\n",
       "          0.50000021,  0.50000021,  0.50000021,  0.50000021,  0.50000021,\n",
       "          0.50000021,  0.50000021]]),\n",
       " array([[ 0.4999998,  0.4999998,  0.4999998,  0.4999998,  0.4999998,\n",
       "          0.4999998,  0.4999998,  0.4999998,  0.4999998,  0.4999998,\n",
       "          0.4999998,  0.4999998,  0.4999998,  0.4999998,  0.4999998,\n",
       "          0.4999998,  0.4999998,  0.4999998,  0.4999998,  0.4999998,\n",
       "          0.4999998,  0.4999998],\n",
       "        [ 0.5000002,  0.5000002,  0.5000002,  0.5000002,  0.5000002,\n",
       "          0.5000002,  0.5000002,  0.5000002,  0.5000002,  0.5000002,\n",
       "          0.5000002,  0.5000002,  0.5000002,  0.5000002,  0.5000002,\n",
       "          0.5000002,  0.5000002,  0.5000002,  0.5000002,  0.5000002,\n",
       "          0.5000002,  0.5000002]]),\n",
       " array([[ 0.49999975,  0.49999975,  0.49999975,  0.49999975,  0.49999975,\n",
       "          0.49999975,  0.49999975,  0.49999975,  0.49999975,  0.49999975,\n",
       "          0.49999975,  0.49999975,  0.49999975,  0.49999975,  0.49999975,\n",
       "          0.49999975,  0.49999975,  0.49999975,  0.49999975,  0.49999975,\n",
       "          0.49999975,  0.49999975],\n",
       "        [ 0.50000025,  0.50000025,  0.50000025,  0.50000025,  0.50000025,\n",
       "          0.50000025,  0.50000025,  0.50000025,  0.50000025,  0.50000025,\n",
       "          0.50000025,  0.50000025,  0.50000025,  0.50000025,  0.50000025,\n",
       "          0.50000025,  0.50000025,  0.50000025,  0.50000025,  0.50000025,\n",
       "          0.50000025,  0.50000025]]),\n",
       " array([[ 0.49999966,  0.49999966,  0.49999966,  0.49999966,  0.49999966,\n",
       "          0.49999966,  0.49999966,  0.49999966,  0.49999966,  0.49999966,\n",
       "          0.49999966,  0.49999966,  0.49999966,  0.49999966,  0.49999966,\n",
       "          0.49999966,  0.49999966,  0.49999966,  0.49999966,  0.49999966,\n",
       "          0.49999966,  0.49999966],\n",
       "        [ 0.50000034,  0.50000034,  0.50000034,  0.50000034,  0.50000034,\n",
       "          0.50000034,  0.50000034,  0.50000034,  0.50000034,  0.50000034,\n",
       "          0.50000034,  0.50000034,  0.50000034,  0.50000034,  0.50000034,\n",
       "          0.50000034,  0.50000034,  0.50000034,  0.50000034,  0.50000034,\n",
       "          0.50000034,  0.50000034]]),\n",
       " array([[ 0.49999967,  0.49999967,  0.49999967,  0.49999967,  0.49999967,\n",
       "          0.49999967,  0.49999967,  0.49999967,  0.49999967,  0.49999967,\n",
       "          0.49999967,  0.49999967,  0.49999967,  0.49999967,  0.49999967,\n",
       "          0.49999967,  0.49999967,  0.49999967,  0.49999967,  0.49999967,\n",
       "          0.49999967,  0.49999967],\n",
       "        [ 0.50000033,  0.50000033,  0.50000033,  0.50000033,  0.50000033,\n",
       "          0.50000033,  0.50000033,  0.50000033,  0.50000033,  0.50000033,\n",
       "          0.50000033,  0.50000033,  0.50000033,  0.50000033,  0.50000033,\n",
       "          0.50000033,  0.50000033,  0.50000033,  0.50000033,  0.50000033,\n",
       "          0.50000033,  0.50000033]])]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_vals[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455],\n",
       "        [ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455]], dtype=float32),\n",
       " array([[ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455],\n",
       "        [ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455]], dtype=float32),\n",
       " array([[ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455],\n",
       "        [ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455]], dtype=float32),\n",
       " array([[ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455],\n",
       "        [ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455]], dtype=float32),\n",
       " array([[ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455],\n",
       "        [ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455]], dtype=float32),\n",
       " array([[ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455],\n",
       "        [ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455]], dtype=float32),\n",
       " array([[ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455],\n",
       "        [ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455]], dtype=float32),\n",
       " array([[ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455],\n",
       "        [ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455]], dtype=float32),\n",
       " array([[ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455],\n",
       "        [ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455]], dtype=float32),\n",
       " array([[ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455],\n",
       "        [ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455]], dtype=float32),\n",
       " array([[ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455],\n",
       "        [ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455]], dtype=float32),\n",
       " array([[ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455],\n",
       "        [ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455]], dtype=float32),\n",
       " array([[ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455],\n",
       "        [ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455]], dtype=float32),\n",
       " array([[ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455],\n",
       "        [ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455]], dtype=float32),\n",
       " array([[ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455],\n",
       "        [ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455]], dtype=float32),\n",
       " array([[ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455],\n",
       "        [ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455]], dtype=float32),\n",
       " array([[ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455],\n",
       "        [ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455]], dtype=float32),\n",
       " array([[ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455],\n",
       "        [ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455]], dtype=float32),\n",
       " array([[ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455],\n",
       "        [ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455]], dtype=float32),\n",
       " array([[ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455],\n",
       "        [ 0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455,  0.04545455,  0.04545455,  0.04545455,\n",
       "          0.04545455,  0.04545455]], dtype=float32)]"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vals[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_num_events': 130,\n",
       " 'events_in': array([[ 9,  9,  4,  4,  4,  4,  5,  4,  4, 10,  1,  9,  4,  9,  4,  9,  8,\n",
       "          8, 18,  7],\n",
       "        [ 4,  4,  4,  9, 10,  9, 12,  1,  6,  4,  8, 18,  8,  4,  4,  9,  4,\n",
       "         10, 11,  4]], dtype=int32),\n",
       " 'events_out': array([[ 9,  4,  4,  4,  4,  5,  4,  4, 10,  1,  9,  4,  9,  4,  9,  8,  8,\n",
       "         18,  7,  4],\n",
       "        [ 4,  4,  9, 10,  9, 12,  1,  6,  4,  8, 18,  8,  4,  4,  9,  4, 10,\n",
       "         11,  4,  9]], dtype=int32),\n",
       " 'initial_state': array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]),\n",
       " 'initial_time': array([ 0.,  0.]),\n",
       " 'times_in': array([[ 0.03780895,  0.05659572,  0.0711524 ,  0.09084344,  0.1167087 ,\n",
       "          0.11749727,  0.11836644,  0.12978144,  0.14427023,  0.14588072,\n",
       "          0.17034163,  0.17376789,  0.19021592,  0.21238699,  0.21258176,\n",
       "          0.21540879,  0.21883103,  0.21883143,  0.21883258,  0.22609817],\n",
       "        [ 0.02668075,  0.03325118,  0.03622794,  0.08866453,  0.09788354,\n",
       "          0.0990687 ,  0.18555974,  0.1870842 ,  0.21152709,  0.21570823,\n",
       "          0.21883037,  0.21883269,  0.22277321,  0.22378501,  0.23709134,\n",
       "          0.26201022,  0.2624901 ,  0.27474064,  0.29912653,  0.30328643]]),\n",
       " 'times_out': array([[ 0.05659572,  0.0711524 ,  0.09084344,  0.1167087 ,  0.11749727,\n",
       "          0.11836644,  0.12978144,  0.14427023,  0.14588072,  0.17034163,\n",
       "          0.17376789,  0.19021592,  0.21238699,  0.21258176,  0.21540879,\n",
       "          0.21883103,  0.21883143,  0.21883258,  0.22609817,  0.25679366],\n",
       "        [ 0.03325118,  0.03622794,  0.08866453,  0.09788354,  0.0990687 ,\n",
       "          0.18555974,  0.1870842 ,  0.21152709,  0.21570823,  0.21883037,\n",
       "          0.21883269,  0.22277321,  0.22378501,  0.23709134,  0.26201022,\n",
       "          0.2624901 ,  0.27474064,  0.29912653,  0.30328643,  0.31445738]])}"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_d[rmtpp_mdl.events_in][:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00208864, -0.0195967 , -0.01328186,  0.00196861],\n",
       "       [-0.01012831,  0.00314247, -0.00908024, -0.01412304]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wem[f_d[rmtpp_mdl.events_in][:, 0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00234153, -0.00234137,  0.01579213,  0.00767435],\n",
       "       [-0.00469474,  0.0054256 , -0.00463418, -0.0046573 ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_lookup fails silently on out of bounds access\n",
    "rmtpp_mdl.sess.run(tf.nn.embedding_lookup(rmtpp_mdl.Wem, [1, 2, -1, 1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const:0' shape=(16, 23) dtype=int64>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RMTPP/initial_state:0',\n",
       " 'RMTPP/initial_time:0',\n",
       " 'RMTPP/events_in:0',\n",
       " 'RMTPP/events_out:0',\n",
       " 'RMTPP/times_in:0',\n",
       " 'RMTPP/times_out:0',\n",
       " 'RMTPP/bptt_events:0']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.name for x in f_d.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 25, 68])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmtpp_mdl.sess.run(tf.gather_nd(tf.constant(np.arange(16 * 23).reshape(16, 23)), \n",
    "                                tf.concat([\n",
    "                                    tf.expand_dims(tf.range(3), -1),\n",
    "                                    tf.mod(tf.constant([[1], [2], [-1]]), 23)\n",
    "                                ], axis=1)\n",
    "                               )\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch... 0\n",
      "Loss during batch 0 last BPTT = 1.814, lr = 0.09965\n",
      "Loss during batch 10 last BPTT = -1.230, lr = 0.09629\n",
      "Loss during batch 20 last BPTT = -1.384, lr = 0.09315\n",
      "Loss during batch 30 last BPTT = -1.347, lr = 0.09021\n",
      "Loss during batch 40 last BPTT = -1.528, lr = 0.08745\n",
      "Loss during batch 50 last BPTT = -1.733, lr = 0.08485\n",
      "Loss during batch 60 last BPTT = -1.395, lr = 0.08241\n",
      "Loss during batch 70 last BPTT = -1.251, lr = 0.08010\n",
      "Loss during batch 80 last BPTT = -1.414, lr = 0.07791\n",
      "Loss on last epoch = -1.3227, new lr = 0.07770, global_step = 2870\n",
      "Model saved at ./save.rmtpp/model.ckpt\n",
      "Running evaluation on training data: ...\n",
      "** MAE = 0.010; valid = 377855, ACC = 0.435\n",
      "CPU times: user 4min 29s, sys: 53.5 s, total: 5min 22s\n",
      "Wall time: 10min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rmtpp_mdl.train(training_data=data, restart=False, \n",
    "                with_summaries=True, num_epochs=1, with_evals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring from ./save.rmtpp/model.ckpt-2870\n",
      "INFO:tensorflow:Restoring parameters from ./save.rmtpp/model.ckpt-2870\n",
      "Starting epoch... 1\n",
      "Loss during batch 0 last BPTT = -1.392, lr = 0.07749\n",
      "Loss during batch 10 last BPTT = -1.508, lr = 0.07544\n",
      "Loss during batch 20 last BPTT = -1.447, lr = 0.07350\n",
      "Loss during batch 30 last BPTT = -1.491, lr = 0.07166\n",
      "Loss during batch 40 last BPTT = -1.496, lr = 0.06991\n",
      "Loss during batch 50 last BPTT = -1.659, lr = 0.06824\n",
      "Loss during batch 60 last BPTT = -1.482, lr = 0.06664\n",
      "Loss during batch 70 last BPTT = -1.268, lr = 0.06513\n",
      "Loss during batch 80 last BPTT = -1.485, lr = 0.06367\n",
      "Loss on last epoch = -1.4179, new lr = 0.06353, global_step = 5740\n",
      "Starting epoch... 2\n",
      "Loss during batch 0 last BPTT = -1.439, lr = 0.06339\n",
      "Loss during batch 10 last BPTT = -1.355, lr = 0.06202\n",
      "Loss during batch 20 last BPTT = -1.375, lr = 0.06070\n",
      "Loss during batch 30 last BPTT = -1.532, lr = 0.05944\n",
      "Loss during batch 40 last BPTT = -1.392, lr = 0.05822\n",
      "Loss during batch 50 last BPTT = -1.592, lr = 0.05706\n",
      "Loss during batch 60 last BPTT = -1.570, lr = 0.05594\n",
      "Loss during batch 70 last BPTT = -1.346, lr = 0.05487\n",
      "Loss during batch 80 last BPTT = -1.430, lr = 0.05384\n",
      "Loss on last epoch = -1.4483, new lr = 0.05373, global_step = 8610\n",
      "Starting epoch... 3\n",
      "Loss during batch 0 last BPTT = -1.733, lr = 0.05363\n",
      "Loss during batch 10 last BPTT = -1.473, lr = 0.05265\n",
      "Loss during batch 20 last BPTT = -1.403, lr = 0.05169\n",
      "Loss during batch 30 last BPTT = -1.654, lr = 0.05077\n",
      "Loss during batch 40 last BPTT = -1.487, lr = 0.04989\n",
      "Loss during batch 50 last BPTT = -1.430, lr = 0.04903\n",
      "Loss during batch 60 last BPTT = -1.439, lr = 0.04820\n",
      "Loss during batch 70 last BPTT = -1.462, lr = 0.04740\n",
      "Loss during batch 80 last BPTT = -1.378, lr = 0.04663\n",
      "Loss on last epoch = -1.4499, new lr = 0.04655, global_step = 11480\n",
      "Starting epoch... 4\n",
      "Loss during batch 0 last BPTT = -1.397, lr = 0.04648\n",
      "Loss during batch 10 last BPTT = -1.459, lr = 0.04574\n",
      "Loss during batch 20 last BPTT = -1.427, lr = 0.04501\n",
      "Loss during batch 30 last BPTT = -1.457, lr = 0.04432\n",
      "Loss during batch 40 last BPTT = -1.484, lr = 0.04364\n",
      "Loss during batch 50 last BPTT = -1.547, lr = 0.04298\n",
      "Loss during batch 60 last BPTT = -0.999, lr = 0.04235\n",
      "Loss during batch 70 last BPTT = -1.715, lr = 0.04173\n",
      "Loss during batch 80 last BPTT = -1.225, lr = 0.04113\n",
      "Loss on last epoch = -1.4592, new lr = 0.04107, global_step = 14350\n",
      "Starting epoch... 5\n",
      "Loss during batch 0 last BPTT = -1.762, lr = 0.04101\n",
      "Loss during batch 10 last BPTT = -1.618, lr = 0.04043\n",
      "Loss during batch 20 last BPTT = -1.508, lr = 0.03986\n",
      "Loss during batch 30 last BPTT = -1.421, lr = 0.03932\n",
      "Loss during batch 40 last BPTT = -1.507, lr = 0.03878\n",
      "Loss during batch 50 last BPTT = -1.338, lr = 0.03826\n",
      "Loss during batch 60 last BPTT = -1.300, lr = 0.03776\n",
      "Loss during batch 70 last BPTT = -1.281, lr = 0.03726\n",
      "Loss during batch 80 last BPTT = -1.509, lr = 0.03678\n",
      "Loss on last epoch = -1.4653, new lr = 0.03674, global_step = 17220\n",
      "Starting epoch... 6\n",
      "Loss during batch 0 last BPTT = -1.367, lr = 0.03669\n",
      "Loss during batch 10 last BPTT = -1.421, lr = 0.03623\n",
      "Loss during batch 20 last BPTT = -1.373, lr = 0.03577\n",
      "Loss during batch 30 last BPTT = -1.571, lr = 0.03533\n",
      "Loss during batch 40 last BPTT = -1.624, lr = 0.03490\n",
      "Loss during batch 50 last BPTT = -1.497, lr = 0.03448\n",
      "Loss during batch 60 last BPTT = -1.464, lr = 0.03407\n",
      "Loss during batch 70 last BPTT = -1.580, lr = 0.03366\n",
      "Loss during batch 80 last BPTT = -1.292, lr = 0.03327\n",
      "Loss on last epoch = 16004.2917, new lr = 0.03323, global_step = 20090\n",
      "Starting epoch... 7\n",
      "Loss during batch 0 last BPTT = -1.445, lr = 0.03320\n",
      "Loss during batch 10 last BPTT = -1.640, lr = 0.03281\n",
      "Loss during batch 20 last BPTT = -1.182, lr = 0.03244\n",
      "Loss during batch 30 last BPTT = -1.493, lr = 0.03208\n",
      "Loss during batch 40 last BPTT = -1.660, lr = 0.03172\n",
      "Loss during batch 50 last BPTT = -1.437, lr = 0.03137\n",
      "Loss during batch 60 last BPTT = -1.565, lr = 0.03103\n",
      "Loss during batch 70 last BPTT = -1.568, lr = 0.03070\n",
      "Loss during batch 80 last BPTT = -1.407, lr = 0.03037\n",
      "Loss on last epoch = -1.4914, new lr = 0.03034, global_step = 22960\n",
      "Starting epoch... 8\n",
      "Loss during batch 0 last BPTT = -1.581, lr = 0.03031\n",
      "Loss during batch 10 last BPTT = -1.430, lr = 0.02999\n",
      "Loss during batch 20 last BPTT = -1.468, lr = 0.02968\n",
      "Loss during batch 30 last BPTT = -1.249, lr = 0.02937\n",
      "Loss during batch 40 last BPTT = -1.864, lr = 0.02907\n",
      "Loss during batch 50 last BPTT = -1.535, lr = 0.02878\n",
      "Loss during batch 60 last BPTT = -1.402, lr = 0.02849\n",
      "Loss during batch 70 last BPTT = -1.477, lr = 0.02821\n",
      "Loss during batch 80 last BPTT = -1.688, lr = 0.02794\n",
      "Loss on last epoch = -1.5154, new lr = 0.02791, global_step = 25830\n",
      "Starting epoch... 9\n",
      "Loss during batch 0 last BPTT = -1.307, lr = 0.02788\n",
      "Loss during batch 10 last BPTT = -1.399, lr = 0.02761\n",
      "Loss during batch 20 last BPTT = -1.619, lr = 0.02735\n",
      "Loss during batch 30 last BPTT = -1.703, lr = 0.02709\n",
      "Loss during batch 40 last BPTT = -1.529, lr = 0.02683\n",
      "Loss during batch 50 last BPTT = -1.736, lr = 0.02659\n",
      "Loss during batch 60 last BPTT = -1.566, lr = 0.02634\n",
      "Loss during batch 70 last BPTT = -1.497, lr = 0.02610\n",
      "Loss during batch 80 last BPTT = -1.294, lr = 0.02586\n",
      "Loss on last epoch = -1.5080, new lr = 0.02584, global_step = 28700\n",
      "Starting epoch... 10\n",
      "Loss during batch 0 last BPTT = -1.505, lr = 0.02582\n",
      "Loss during batch 10 last BPTT = -1.504, lr = 0.02559\n",
      "Loss during batch 20 last BPTT = -1.581, lr = 0.02536\n",
      "Loss during batch 30 last BPTT = -1.472, lr = 0.02514\n",
      "Loss during batch 40 last BPTT = -1.307, lr = 0.02492\n",
      "Loss during batch 50 last BPTT = -1.445, lr = 0.02470\n",
      "Loss during batch 60 last BPTT = -1.635, lr = 0.02449\n",
      "Loss during batch 70 last BPTT = -1.339, lr = 0.02428\n",
      "Loss during batch 80 last BPTT = -1.690, lr = 0.02408\n",
      "Loss on last epoch = -1.5497, new lr = 0.02406, global_step = 31570\n",
      "Model saved at ./save.rmtpp/model.ckpt\n",
      "Running evaluation on training data: ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:571: RuntimeWarning: overflow encountered in exp\n",
      "  C = np.exp(np.dot(h_i, Vt) + bt).reshape(-1)\n",
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t + (c / w) * (np.exp(-w * t) - 1))\n",
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:364: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  warnings.warn(msg, IntegrationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** MAE = 0.014; valid = 108165, ACC = 0.001\n",
      "CPU times: user 35min 29s, sys: 4min 33s, total: 40min 2s\n",
      "Wall time: 35min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rmtpp_mdl.train(training_data=data, restart=True, with_summaries=True, num_epochs=10, with_evals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring from ./save.rmtpp/model.ckpt-31570\n",
      "INFO:tensorflow:Restoring parameters from ./save.rmtpp/model.ckpt-31570\n",
      "Starting epoch... 11\n",
      "Loss during batch 0 last BPTT = -1.462, lr = 0.02404\n",
      "Loss during batch 10 last BPTT = -1.679, lr = 0.02384\n",
      "Loss during batch 20 last BPTT = -1.373, lr = 0.02364\n",
      "Loss during batch 30 last BPTT = -1.710, lr = 0.02344\n",
      "Loss during batch 40 last BPTT = -1.640, lr = 0.02325\n",
      "Loss during batch 50 last BPTT = -1.360, lr = 0.02307\n",
      "Loss during batch 60 last BPTT = -1.509, lr = 0.02288\n",
      "Loss during batch 70 last BPTT = -1.593, lr = 0.02270\n",
      "Loss during batch 80 last BPTT = -1.368, lr = 0.02252\n",
      "Loss on last epoch = 38597887186223.0547, new lr = 0.02250, global_step = 34440\n",
      "Starting epoch... 12\n",
      "Loss during batch 0 last BPTT = -1.629, lr = 0.02248\n",
      "Loss during batch 10 last BPTT = -1.542, lr = 0.02231\n",
      "Loss during batch 20 last BPTT = -1.611, lr = 0.02214\n",
      "Loss during batch 30 last BPTT = -1.599, lr = 0.02197\n",
      "Loss during batch 40 last BPTT = -1.380, lr = 0.02180\n",
      "Loss during batch 50 last BPTT = -1.707, lr = 0.02163\n",
      "Loss during batch 60 last BPTT = -1.629, lr = 0.02147\n",
      "Loss during batch 70 last BPTT = -1.356, lr = 0.02131\n",
      "Loss during batch 80 last BPTT = -1.581, lr = 0.02115\n",
      "Loss on last epoch = -1.5741, new lr = 0.02114, global_step = 37310\n",
      "Starting epoch... 13\n",
      "Loss during batch 0 last BPTT = -1.530, lr = 0.02112\n",
      "Loss during batch 10 last BPTT = -1.680, lr = 0.02097\n",
      "Loss during batch 20 last BPTT = -1.644, lr = 0.02081\n",
      "Loss during batch 30 last BPTT = -1.550, lr = 0.02066\n",
      "Loss during batch 40 last BPTT = -1.677, lr = 0.02051\n",
      "Loss during batch 50 last BPTT = -1.843, lr = 0.02037\n",
      "Loss during batch 60 last BPTT = -1.673, lr = 0.02022\n",
      "Loss during batch 70 last BPTT = -1.600, lr = 0.02008\n",
      "Loss during batch 80 last BPTT = -1.525, lr = 0.01994\n",
      "Loss on last epoch = -1.6027, new lr = 0.01993, global_step = 40180\n",
      "Starting epoch... 14\n",
      "Loss during batch 0 last BPTT = -1.415, lr = 0.01991\n",
      "Loss during batch 10 last BPTT = -1.478, lr = 0.01978\n",
      "Loss during batch 20 last BPTT = -1.470, lr = 0.01964\n",
      "Loss during batch 30 last BPTT = -1.486, lr = 0.01951\n",
      "Loss during batch 40 last BPTT = -1.653, lr = 0.01937\n",
      "Loss during batch 50 last BPTT = -1.534, lr = 0.01924\n",
      "Loss during batch 60 last BPTT = -1.434, lr = 0.01911\n",
      "Loss during batch 70 last BPTT = -1.371, lr = 0.01899\n",
      "Loss during batch 80 last BPTT = -1.482, lr = 0.01886\n",
      "Loss on last epoch = -1.5916, new lr = 0.01885, global_step = 43050\n",
      "Starting epoch... 15\n",
      "Loss during batch 0 last BPTT = -1.656, lr = 0.01884\n",
      "Loss during batch 10 last BPTT = -1.689, lr = 0.01871\n",
      "Loss during batch 20 last BPTT = -1.459, lr = 0.01859\n",
      "Loss during batch 30 last BPTT = -1.839, lr = 0.01847\n",
      "Loss during batch 40 last BPTT = -1.805, lr = 0.01835\n",
      "Loss during batch 50 last BPTT = -1.475, lr = 0.01824\n",
      "Loss during batch 60 last BPTT = -1.345, lr = 0.01812\n",
      "Loss during batch 70 last BPTT = -1.682, lr = 0.01801\n",
      "Loss during batch 80 last BPTT = -1.591, lr = 0.01789\n",
      "Loss on last epoch = -1.5929, new lr = 0.01788, global_step = 45920\n",
      "Starting epoch... 16\n",
      "Loss during batch 0 last BPTT = -1.399, lr = 0.01787\n",
      "Loss during batch 10 last BPTT = -1.454, lr = 0.01776\n",
      "Loss during batch 20 last BPTT = -1.799, lr = 0.01765\n",
      "Loss during batch 30 last BPTT = -1.465, lr = 0.01754\n",
      "Loss during batch 40 last BPTT = -1.444, lr = 0.01744\n",
      "Loss during batch 50 last BPTT = -1.820, lr = 0.01733\n",
      "Loss during batch 60 last BPTT = -1.667, lr = 0.01723\n",
      "Loss during batch 70 last BPTT = -1.523, lr = 0.01712\n",
      "Loss during batch 80 last BPTT = -1.467, lr = 0.01702\n",
      "Loss on last epoch = 774004925834945.7500, new lr = 0.01701, global_step = 48790\n",
      "Starting epoch... 17\n",
      "Loss during batch 0 last BPTT = -1.675, lr = 0.01700\n",
      "Loss during batch 10 last BPTT = -1.637, lr = 0.01690\n",
      "Loss during batch 20 last BPTT = -1.525, lr = 0.01680\n",
      "Loss during batch 30 last BPTT = -1.520, lr = 0.01670\n",
      "Loss during batch 40 last BPTT = -1.640, lr = 0.01660\n",
      "Loss during batch 50 last BPTT = -1.445, lr = 0.01651\n",
      "Loss during batch 60 last BPTT = -1.529, lr = 0.01641\n",
      "Loss during batch 70 last BPTT = -1.481, lr = 0.01632\n",
      "Loss during batch 80 last BPTT = -1.796, lr = 0.01623\n",
      "Loss on last epoch = -1.6100, new lr = 0.01622, global_step = 51660\n",
      "Starting epoch... 18\n",
      "Loss during batch 0 last BPTT = -1.518, lr = 0.01621\n",
      "Loss during batch 10 last BPTT = -1.456, lr = 0.01612\n",
      "Loss during batch 20 last BPTT = -1.547, lr = 0.01603\n",
      "Loss during batch 30 last BPTT = -1.618, lr = 0.01594\n",
      "Loss during batch 40 last BPTT = -1.735, lr = 0.01585\n",
      "Loss during batch 50 last BPTT = -1.503, lr = 0.01576\n",
      "Loss during batch 60 last BPTT = -1.628, lr = 0.01568\n",
      "Loss during batch 70 last BPTT = -1.673, lr = 0.01559\n",
      "Loss during batch 80 last BPTT = -1.544, lr = 0.01551\n",
      "Loss on last epoch = -1.6228, new lr = 0.01550, global_step = 54530\n",
      "Starting epoch... 19\n",
      "Loss during batch 0 last BPTT = -1.646, lr = 0.01549\n",
      "Loss during batch 10 last BPTT = -1.440, lr = 0.01540\n",
      "Loss during batch 20 last BPTT = -1.558, lr = 0.01532\n",
      "Loss during batch 30 last BPTT = -1.723, lr = 0.01524\n",
      "Loss during batch 40 last BPTT = -1.403, lr = 0.01516\n",
      "Loss during batch 50 last BPTT = -1.641, lr = 0.01508\n",
      "Loss during batch 60 last BPTT = -1.463, lr = 0.01500\n",
      "Loss during batch 70 last BPTT = -1.649, lr = 0.01492\n",
      "Loss during batch 80 last BPTT = -1.576, lr = 0.01484\n",
      "Loss on last epoch = 270318514574710.1875, new lr = 0.01484, global_step = 57400\n",
      "Starting epoch... 20\n",
      "Loss during batch 0 last BPTT = -1.585, lr = 0.01483\n",
      "Loss during batch 10 last BPTT = -1.338, lr = 0.01475\n",
      "Loss during batch 20 last BPTT = -1.295, lr = 0.01468\n",
      "Loss during batch 30 last BPTT = -1.724, lr = 0.01460\n",
      "Loss during batch 40 last BPTT = -1.563, lr = 0.01453\n",
      "Loss during batch 50 last BPTT = -1.650, lr = 0.01445\n",
      "Loss during batch 60 last BPTT = -1.566, lr = 0.01438\n",
      "Loss during batch 70 last BPTT = -1.675, lr = 0.01431\n",
      "Loss during batch 80 last BPTT = -1.446, lr = 0.01424\n",
      "Loss on last epoch = -1.5988, new lr = 0.01423, global_step = 60270\n",
      "Model saved at ./save.rmtpp/model.ckpt\n",
      "CPU times: user 35min 9s, sys: 4min 8s, total: 39min 18s\n",
      "Wall time: 18min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rmtpp_mdl.train(training_data=data, restart=True, with_summaries=True, num_epochs=10, with_evals=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring from ./save.rmtpp/model.ckpt-60270\n",
      "INFO:tensorflow:Restoring parameters from ./save.rmtpp/model.ckpt-60270\n",
      "Starting epoch... 21\n",
      "Loss during batch 0 last BPTT = -1.546, lr = 0.01422\n",
      "Loss during batch 10 last BPTT = -1.776, lr = 0.01415\n",
      "Loss during batch 20 last BPTT = -1.781, lr = 0.01408\n",
      "Loss during batch 30 last BPTT = -1.555, lr = 0.01401\n",
      "Loss during batch 40 last BPTT = -1.728, lr = 0.01395\n",
      "Loss during batch 50 last BPTT = -1.795, lr = 0.01388\n",
      "Loss during batch 60 last BPTT = -1.690, lr = 0.01381\n",
      "Loss during batch 70 last BPTT = -1.575, lr = 0.01374\n",
      "Loss during batch 80 last BPTT = -1.488, lr = 0.01368\n",
      "Loss on last epoch = -1.5989, new lr = 0.01367, global_step = 63140\n",
      "Starting epoch... 22\n",
      "Loss during batch 0 last BPTT = -1.337, lr = 0.01367\n",
      "Loss during batch 10 last BPTT = -1.898, lr = 0.01360\n",
      "Loss during batch 20 last BPTT = -1.746, lr = 0.01354\n",
      "Loss during batch 30 last BPTT = -1.661, lr = 0.01347\n",
      "Loss during batch 40 last BPTT = -1.820, lr = 0.01341\n",
      "Loss during batch 50 last BPTT = -1.746, lr = 0.01335\n",
      "Loss during batch 60 last BPTT = -1.752, lr = 0.01328\n",
      "Loss during batch 70 last BPTT = -1.653, lr = 0.01322\n",
      "Loss during batch 80 last BPTT = -1.715, lr = 0.01316\n",
      "Loss on last epoch = -1.6067, new lr = 0.01316, global_step = 66010\n",
      "Starting epoch... 23\n",
      "Loss during batch 0 last BPTT = -1.407, lr = 0.01315\n",
      "Loss during batch 10 last BPTT = -1.674, lr = 0.01309\n",
      "Loss during batch 20 last BPTT = -1.572, lr = 0.01303\n",
      "Loss during batch 30 last BPTT = -1.569, lr = 0.01297\n",
      "Loss during batch 40 last BPTT = -1.409, lr = 0.01291\n",
      "Loss during batch 50 last BPTT = -1.618, lr = 0.01285\n",
      "Loss during batch 60 last BPTT = -1.677, lr = 0.01280\n",
      "Loss during batch 70 last BPTT = -1.624, lr = 0.01274\n",
      "Loss during batch 80 last BPTT = -1.826, lr = 0.01268\n",
      "Loss on last epoch = 109479859956319.4219, new lr = 0.01268, global_step = 68880\n",
      "Starting epoch... 24\n",
      "Loss during batch 0 last BPTT = -1.653, lr = 0.01267\n",
      "Loss during batch 10 last BPTT = -1.435, lr = 0.01262\n",
      "Loss during batch 20 last BPTT = -1.577, lr = 0.01256\n",
      "Loss during batch 30 last BPTT = -1.685, lr = 0.01251\n",
      "Loss during batch 40 last BPTT = -1.595, lr = 0.01245\n",
      "Loss during batch 50 last BPTT = -1.462, lr = 0.01240\n",
      "Loss during batch 60 last BPTT = -1.585, lr = 0.01234\n",
      "Loss during batch 70 last BPTT = -1.739, lr = 0.01229\n",
      "Loss during batch 80 last BPTT = -1.484, lr = 0.01224\n",
      "Loss on last epoch = -1.6166, new lr = 0.01223, global_step = 71750\n",
      "Starting epoch... 25\n",
      "Loss during batch 0 last BPTT = -1.473, lr = 0.01223\n",
      "Loss during batch 10 last BPTT = -1.484, lr = 0.01218\n",
      "Loss during batch 20 last BPTT = -1.643, lr = 0.01212\n",
      "Loss during batch 30 last BPTT = -1.675, lr = 0.01207\n",
      "Loss during batch 40 last BPTT = -1.745, lr = 0.01202\n",
      "Loss during batch 50 last BPTT = -1.667, lr = 0.01197\n",
      "Loss during batch 60 last BPTT = -1.885, lr = 0.01192\n",
      "Loss during batch 70 last BPTT = -1.332, lr = 0.01187\n",
      "Loss during batch 80 last BPTT = -1.610, lr = 0.01182\n",
      "Loss on last epoch = -1.6193, new lr = 0.01182, global_step = 74620\n",
      "Starting epoch... 26\n",
      "Loss during batch 0 last BPTT = -1.518, lr = 0.01181\n",
      "Loss during batch 10 last BPTT = -1.661, lr = 0.01176\n",
      "Loss during batch 20 last BPTT = -1.528, lr = 0.01172\n",
      "Loss during batch 30 last BPTT = -1.506, lr = 0.01167\n",
      "Loss during batch 40 last BPTT = -1.814, lr = 0.01162\n",
      "Loss during batch 50 last BPTT = -1.783, lr = 0.01157\n",
      "Loss during batch 60 last BPTT = -1.631, lr = 0.01153\n",
      "Loss during batch 70 last BPTT = -1.545, lr = 0.01148\n",
      "Loss during batch 80 last BPTT = -1.688, lr = 0.01143\n",
      "Loss on last epoch = 4071870.3554, new lr = 0.01143, global_step = 77490\n",
      "Starting epoch... 27\n",
      "Loss during batch 0 last BPTT = -1.659, lr = 0.01143\n",
      "Loss during batch 10 last BPTT = -1.454, lr = 0.01138\n",
      "Loss during batch 20 last BPTT = -1.576, lr = 0.01133\n",
      "Loss during batch 30 last BPTT = -1.785, lr = 0.01129\n",
      "Loss during batch 40 last BPTT = -1.690, lr = 0.01125\n",
      "Loss during batch 50 last BPTT = -1.751, lr = 0.01120\n",
      "Loss during batch 60 last BPTT = -1.655, lr = 0.01116\n",
      "Loss during batch 70 last BPTT = -1.513, lr = 0.01111\n",
      "Loss during batch 80 last BPTT = -1.958, lr = 0.01107\n",
      "Loss on last epoch = -1.6164, new lr = 0.01107, global_step = 80360\n",
      "Starting epoch... 28\n",
      "Loss during batch 0 last BPTT = -1.617, lr = 0.01106\n",
      "Loss during batch 10 last BPTT = -1.683, lr = 0.01102\n",
      "Loss during batch 20 last BPTT = -1.443, lr = 0.01098\n",
      "Loss during batch 30 last BPTT = -1.370, lr = 0.01094\n",
      "Loss during batch 40 last BPTT = -1.616, lr = 0.01089\n",
      "Loss during batch 50 last BPTT = -1.586, lr = 0.01085\n",
      "Loss during batch 60 last BPTT = -1.619, lr = 0.01081\n",
      "Loss during batch 70 last BPTT = -1.679, lr = 0.01077\n",
      "Loss during batch 80 last BPTT = -1.600, lr = 0.01073\n",
      "Loss on last epoch = 658433546315238.8750, new lr = 0.01073, global_step = 83230\n",
      "Starting epoch... 29\n",
      "Loss during batch 0 last BPTT = -1.880, lr = 0.01072\n",
      "Loss during batch 10 last BPTT = -1.717, lr = 0.01068\n",
      "Loss during batch 20 last BPTT = -1.641, lr = 0.01064\n",
      "Loss during batch 30 last BPTT = -1.381, lr = 0.01060\n",
      "Loss during batch 40 last BPTT = -1.916, lr = 0.01056\n",
      "Loss during batch 50 last BPTT = -1.516, lr = 0.01052\n",
      "Loss during batch 60 last BPTT = -1.565, lr = 0.01049\n",
      "Loss during batch 70 last BPTT = -1.716, lr = 0.01045\n",
      "Loss during batch 80 last BPTT = -1.553, lr = 0.01041\n",
      "Loss on last epoch = -1.6125, new lr = 0.01041, global_step = 86100\n",
      "Starting epoch... 30\n",
      "Loss during batch 0 last BPTT = -1.481, lr = 0.01040\n",
      "Loss during batch 10 last BPTT = -1.679, lr = 0.01036\n",
      "Loss during batch 20 last BPTT = -1.401, lr = 0.01033\n",
      "Loss during batch 30 last BPTT = -1.879, lr = 0.01029\n",
      "Loss during batch 40 last BPTT = -1.852, lr = 0.01025\n",
      "Loss during batch 50 last BPTT = -1.497, lr = 0.01022\n",
      "Loss during batch 60 last BPTT = -1.488, lr = 0.01018\n",
      "Loss during batch 70 last BPTT = -1.562, lr = 0.01014\n",
      "Loss during batch 80 last BPTT = -2.061, lr = 0.01011\n",
      "Loss on last epoch = -1.6280, new lr = 0.01010, global_step = 88970\n",
      "Model saved at ./save.rmtpp/model.ckpt\n",
      "CPU times: user 34min 55s, sys: 4min 1s, total: 38min 57s\n",
      "Wall time: 18min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rmtpp_mdl.train(training_data=data, restart=True, with_summaries=True, num_epochs=10, with_evals=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "         nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "         nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "         nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "         nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "         nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan]], dtype=float32)"
      ]
     },
     "execution_count": 647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmtpp_mdl.sess.run(rmtpp_mdl.bh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013124610843665211"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((data['train_time_out_seq'] - data['train_time_in_seq'])[data['train_event_in_seq'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring from ./save.rmtpp/model.ckpt-4389\n",
      "INFO:tensorflow:Restoring parameters from ./save.rmtpp/model.ckpt-4389\n",
      "Starting epoch... 11\n",
      "Loss during batch 0 last BPTT = -184487973179453389661792108544.000, lr = 0.06762\n",
      "Loss on last epoch = -184487973179453389661792108544.0000, new lr = 0.06762, global_step = 4788\n",
      "Starting epoch... 12\n",
      "Loss during batch 0 last BPTT = -187383577181241474737395204096.000, lr = 0.06585\n",
      "Loss on last epoch = -187383577181241474737395204096.0000, new lr = 0.06585, global_step = 5187\n",
      "Starting epoch... 13\n",
      "Loss during batch 0 last BPTT = -130741284654747086018544926720.000, lr = 0.06416\n",
      "Loss on last epoch = -130741284654747086018544926720.0000, new lr = 0.06416, global_step = 5586\n",
      "Starting epoch... 14\n",
      "Loss during batch 0 last BPTT = -152195260040927101385657286656.000, lr = 0.06256\n",
      "Loss on last epoch = -152195260040927101385657286656.0000, new lr = 0.06256, global_step = 5985\n",
      "Starting epoch... 15\n",
      "Loss during batch 0 last BPTT = -162345023409074357127176257536.000, lr = 0.06104\n",
      "Loss on last epoch = -162345023409074357127176257536.0000, new lr = 0.06104, global_step = 6384\n",
      "Starting epoch... 16\n",
      "Loss during batch 0 last BPTT = -205807379997454441934848262144.000, lr = 0.05958\n",
      "Loss on last epoch = -205807379997454441934848262144.0000, new lr = 0.05958, global_step = 6783\n",
      "Starting epoch... 17\n",
      "Loss during batch 0 last BPTT = -187231403650317211792310272000.000, lr = 0.05820\n",
      "Loss on last epoch = -187231403650317211792310272000.0000, new lr = 0.05820, global_step = 7182\n",
      "Starting epoch... 18\n",
      "Loss during batch 0 last BPTT = -165194272674686628996531093504.000, lr = 0.05688\n",
      "Loss on last epoch = -165194272674686628996531093504.0000, new lr = 0.05688, global_step = 7581\n",
      "Starting epoch... 19\n",
      "Loss during batch 0 last BPTT = -165607744244777511964635037696.000, lr = 0.05562\n",
      "Loss on last epoch = -165607744244777511964635037696.0000, new lr = 0.05562, global_step = 7980\n",
      "Starting epoch... 20\n",
      "Loss during batch 0 last BPTT = -152301872194271212464953622528.000, lr = 0.05441\n",
      "Loss on last epoch = -152301872194271212464953622528.0000, new lr = 0.05441, global_step = 8379\n",
      "Model saved at ./save.rmtpp/model.ckpt\n",
      "CPU times: user 4min 3s, sys: 2min 11s, total: 6min 14s\n",
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rmtpp_mdl.train(training_data=data, restart=True, with_summaries=True, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.95 s, sys: 5.35 s, total: 13.3 s\n",
      "Wall time: 6min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_time_preds, train_events_preds = rmtpp_mdl.predict_train(data=data, \n",
    "                                                               single_threaded=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.44 s, sys: 3.27 s, total: 5.71 s\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_time_preds, test_events_preds = rmtpp_mdl.predict_test(data=data, \n",
    "                                                            single_threaded=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02738013,  0.01602918,  0.02759087,  0.02933192,  0.03105356,\n",
       "        0.03656282,  0.05459098,  0.06327584,  0.06608187,  0.06938239,\n",
       "        0.07430207,  0.08015435,  0.08423739,  0.08486586,  0.09088981,\n",
       "        0.09156575,  0.09267612,  0.10201938,  0.10365391,  0.12228292,\n",
       "        0.12386437,  0.1264394 ,  0.12795465,  0.13031584,  0.15695599,\n",
       "        0.1669771 ,  0.1672006 ,  0.16915276,  0.17132413,  0.17204702,\n",
       "        0.17723013,  0.17967722,  0.18244779,  0.184943  ,  0.19656371,\n",
       "        0.21381454,  0.21672656,  0.22469257,  0.22745849,  0.22771935,\n",
       "        0.22326078,  0.23091319,  0.2383613 ,  0.24303566,  0.24523483,\n",
       "        0.2472588 ,  0.2483997 ,  0.25538281,  0.25638071,  0.26710355,\n",
       "        0.28346782,  0.29021445,  0.29539624,  0.29781487,  0.30148874,\n",
       "        0.31309213,  0.32341733,  0.33141566,  0.33217156,  0.33293195,\n",
       "        0.33346916,  0.33992742,  0.3401174 ,  0.34073554,  0.35013183,\n",
       "        0.35066851,  0.35177533,  0.35297293,  0.35546293,  0.36399207,\n",
       "        0.36621385,  0.36738833,  0.37138006,  0.37138951,  0.37497164,\n",
       "        0.37949376,  0.39961383,  0.4043934 ,  0.40598978,  0.40810866,\n",
       "        0.41196566,  0.41636545,  0.4218661 ,  0.43895696,  0.43977442,\n",
       "        0.44390312,  0.45046493,  0.46563339,  0.48137513,  0.48630269,\n",
       "        0.48908733,  0.51017808,  0.51647941,  0.52337138,  0.52593932,\n",
       "        0.54240155,  0.54240633,  0.54379351,  0.54460154,  0.56178842,\n",
       "        0.55965825,  0.56573494,  0.57058818,  0.57077831,  0.57161028,\n",
       "        0.57974344,  0.58866436,  0.59297603,  0.59872706,  0.60161356,\n",
       "        0.60821472,  0.60979747,  0.61814427,  0.61864814,  0.62735454,\n",
       "        0.62846807,  0.63050216,  0.63107761,  0.6322892 ,  0.63619628,\n",
       "        0.63667197,  0.63834616,  0.64583321,  0.64646149,  0.64794971,\n",
       "        0.64881541,  0.65463368,  0.65716743,  0.66545766,  0.67127216,\n",
       "        0.67135801,  0.67447715,  0.67892922,  0.68127102,  0.68442067,\n",
       "        0.68484378,  0.68691928,  0.68991404,  0.69409883,  0.69884083,\n",
       "        0.70645695,  0.70657631,  0.70915701,  0.71023658,  0.71592088,\n",
       "        0.71663376,  0.72247539,  0.73353839,  0.73795499,  0.7419955 ,\n",
       "        0.74311269,  0.74311758,  0.74365952,  0.75031066,  0.75635067,\n",
       "        0.76570911,  0.77010806,  0.77413649,  0.77422822,  0.79260252,\n",
       "        0.7941309 ,  0.79493938,  0.81217216,  0.81748105,  0.82037738,\n",
       "        0.82742828,  0.83021129,  0.83504641,  0.83522289,  0.83580789,\n",
       "        0.83648714,  0.84208628,  0.84220544,  0.84974461,  0.85040495,\n",
       "        0.85679092,  0.86406788,  0.86519499,  0.8674822 ,  0.86786232,\n",
       "        0.86866092,  0.87062004,  0.87748863,  0.87875298,  0.87960892,\n",
       "        0.88044049,  0.88074028,  0.8815003 ,  0.88407628,  0.88993854,\n",
       "        0.89083206,  0.89842119,  0.89949997,  0.90615549,  0.90721102,\n",
       "        0.90769619,  0.90955635,  0.91146298,  0.91213374,  0.91279931,\n",
       "        0.91591091,  0.91859623,  0.921126  ,  0.92387857,  0.92672167,\n",
       "        0.93097204,  0.93120491,  0.93908144,  0.94440344,  0.95333617,\n",
       "        0.96636151,  0.9673596 ,  0.96751639,  0.96786777,  0.97348205,\n",
       "        0.97474255,  0.97553606,  0.97674444,  0.97689651,  0.97717189,\n",
       "        0.97679767,  0.97782386,  0.98131826,  0.98614454,  0.98632605,\n",
       "        0.99029161,  0.99439855,  0.99714799,  1.00316133,  1.00615019,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244,\n",
       "        0.01208244,  0.01208244,  0.01208244,  0.01208244,  0.01208244])"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_time_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD4CAYAAAANbUbJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe0f4b91dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.arange(0.01, 5, 0.1)\n",
    "c, w = 1.0, -1.0\n",
    "plt.plot(X, [tf_rmtpp.rmtpp_core.quad_func(t, c, w) / t for t in X])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train_event_in_seq', 'train_event_out_seq', 'train_time_in_seq', 'train_time_out_seq', 'test_event_in_seq', 'test_event_out_seq', 'test_time_in_seq', 'test_time_out_seq', 'num_categories'])"
      ]
     },
     "execution_count": 807,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.011261327784789284, 95902)"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_rmtpp.utils.MAE(test_time_preds, \n",
    "                   data['test_time_out_seq'], \n",
    "                   data['test_event_out_seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.011286010419434762, 377865)"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_rmtpp.utils.MAE(train_time_preds, \n",
    "                   data['train_time_out_seq'], \n",
    "                   data['train_event_out_seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39524697974144207"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_rmtpp.utils.ACC(train_events_preds, \n",
    "                   data['train_event_out_seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3879898229442556"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_rmtpp.utils.ACC(test_events_preds, \n",
    "                   data['test_event_out_seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num categories =  22\n",
      "delta-t (training) = \n",
      "count    3.778740e+05\n",
      "mean     1.312461e-02\n",
      "std      1.652969e-02\n",
      "min      2.058333e-10\n",
      "25%      2.613212e-03\n",
      "50%      7.631856e-03\n",
      "75%      1.736150e-02\n",
      "max      3.220176e-01\n",
      "dtype: float64\n",
      "base-rate = 76.19273530557021, log(base_rate) = 4.33326612095048\n",
      "Class probs =  [  6.24255704e-02   1.97711406e-02   6.48364270e-03   4.36052758e-01\n",
      "   5.75456369e-02   6.17480959e-02   1.94244642e-02   1.92365709e-02\n",
      "   2.29195446e-01   1.39199839e-02   2.19914575e-03   2.74800595e-02\n",
      "   8.77805829e-03   1.88898945e-02   3.03011057e-03   9.92394290e-04\n",
      "   2.77341124e-03   5.37745386e-03   3.75257361e-03   7.14523889e-04\n",
      "   1.50843932e-04   5.82204650e-05]\n",
      "delta-t (testing) = \n",
      "count    9.590700e+04\n",
      "mean     1.291405e-02\n",
      "std      1.646905e-02\n",
      "min      2.058333e-10\n",
      "25%      2.424433e-03\n",
      "50%      7.344151e-03\n",
      "75%      1.706356e-02\n",
      "max      2.712632e-01\n",
      "dtype: float64\n",
      "base-rate = 77.43506973871634, log(base_rate) = 4.349439775377487\n",
      "Class probs =  [  6.24667647e-02   2.13435933e-02   7.06934843e-03   4.30688062e-01\n",
      "   5.79936814e-02   6.49587621e-02   2.00298206e-02   1.87264746e-02\n",
      "   2.28523465e-01   1.27832171e-02   2.03321968e-03   2.71617296e-02\n",
      "   8.39354792e-03   2.01862221e-02   3.13845705e-03   1.04267676e-03\n",
      "   2.79437372e-03   5.89112369e-03   3.86833078e-03   6.04752521e-04\n",
      "   2.29388887e-04   7.29873732e-05]\n",
      "Training sequence lenghts = \n",
      "count    5307.000000\n",
      "mean       71.202940\n",
      "std        42.446415\n",
      "min        40.000000\n",
      "25%        46.000000\n",
      "50%        57.000000\n",
      "75%        79.000000\n",
      "max       719.000000\n",
      "dtype: float64\n",
      "Testing sequence lenghts = \n",
      "count    1326.000000\n",
      "mean       72.328054\n",
      "std        48.312648\n",
      "min        40.000000\n",
      "25%        46.000000\n",
      "50%        57.000000\n",
      "75%        79.000000\n",
      "max       735.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "tf_rmtpp.utils.data_stats(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import Pdb\n",
    "pdb = Pdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmtpp_mdl.sess.run(tf.nn.softplus(rmtpp_mdl.wt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_rmtpp.rmtpp_core.softplus(rmtpp_mdl.sess.run(rmtpp_mdl.wt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py\u001b[0m(574)\u001b[0;36mpredict_train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    572 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mpredict_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_threaded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    573 \u001b[0;31m        \u001b[0;34m\"\"\"Make (time, event) predictions on the training data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 574 \u001b[0;31m        return self.predict(event_in_seq=data['train_event_in_seq'],\n",
      "\u001b[0m\u001b[0;32m    575 \u001b[0;31m                            \u001b[0mtime_in_seq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_time_in_seq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    576 \u001b[0;31m                            single_threaded=single_threaded)\n",
      "\u001b[0m\n",
      "ipdb> until 551\n",
      "*** \"until\" line number is smaller than current line number\n",
      "ipdb> s\n",
      "> \u001b[0;32m/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py\u001b[0m(575)\u001b[0;36mpredict_train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    572 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mpredict_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_threaded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    573 \u001b[0;31m        \u001b[0;34m\"\"\"Make (time, event) predictions on the training data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    574 \u001b[0;31m        return self.predict(event_in_seq=data['train_event_in_seq'],\n",
      "\u001b[0m\u001b[0;32m--> 575 \u001b[0;31m                            \u001b[0mtime_in_seq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_time_in_seq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    576 \u001b[0;31m                            single_threaded=single_threaded)\n",
      "\u001b[0m\n",
      "ipdb> s\n",
      "> \u001b[0;32m/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py\u001b[0m(576)\u001b[0;36mpredict_train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    572 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mpredict_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_threaded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    573 \u001b[0;31m        \u001b[0;34m\"\"\"Make (time, event) predictions on the training data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    574 \u001b[0;31m        return self.predict(event_in_seq=data['train_event_in_seq'],\n",
      "\u001b[0m\u001b[0;32m    575 \u001b[0;31m                            \u001b[0mtime_in_seq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_time_in_seq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 576 \u001b[0;31m                            single_threaded=single_threaded)\n",
      "\u001b[0m\n",
      "ipdb> s\n",
      "--Call--\n",
      "> \u001b[0;32m/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py\u001b[0m(502)\u001b[0;36mpredict\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    500 \u001b[0;31m        \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    501 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 502 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_in_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_in_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_threaded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    503 \u001b[0;31m        \u001b[0;34m\"\"\"Treats the entire dataset as a single batch and processes it.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    504 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> until 555\n",
      "> \u001b[0;32m/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py\u001b[0m(558)\u001b[0;36mpredict\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    556 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mpreds_i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    557 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 558 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0msingle_threaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    559 \u001b[0;31m            \u001b[0mall_time_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_quad_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    560 \u001b[0;31m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> s\n",
      "> \u001b[0;32m/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py\u001b[0m(559)\u001b[0;36mpredict\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    557 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    558 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0msingle_threaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 559 \u001b[0;31m            \u001b[0mall_time_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_quad_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    560 \u001b[0;31m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    561 \u001b[0;31m            \u001b[0;32mwith\u001b[0m \u001b[0mMP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> s\n",
      "--Call--\n",
      "> \u001b[0;32m/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py\u001b[0m(559)\u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    557 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    558 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0msingle_threaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 559 \u001b[0;31m            \u001b[0mall_time_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_quad_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    560 \u001b[0;31m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    561 \u001b[0;31m            \u001b[0;32mwith\u001b[0m \u001b[0mMP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> s\n",
      "> \u001b[0;32m/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py\u001b[0m(559)\u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    557 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    558 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0msingle_threaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 559 \u001b[0;31m            \u001b[0mall_time_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_quad_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    560 \u001b[0;31m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    561 \u001b[0;31m            \u001b[0;32mwith\u001b[0m \u001b[0mMP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> s\n",
      "--Call--\n",
      "> \u001b[0;32m/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py\u001b[0m(546)\u001b[0;36m_quad_worker\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    544 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    545 \u001b[0;31m        \u001b[0;32mglobal\u001b[0m \u001b[0m_quad_worker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 546 \u001b[0;31m        \u001b[0;32mdef\u001b[0m \u001b[0m_quad_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    547 \u001b[0;31m            \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    548 \u001b[0;31m            \u001b[0mpreds_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> s\n",
      "> \u001b[0;32m/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py\u001b[0m(547)\u001b[0;36m_quad_worker\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    545 \u001b[0;31m        \u001b[0;32mglobal\u001b[0m \u001b[0m_quad_worker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    546 \u001b[0;31m        \u001b[0;32mdef\u001b[0m \u001b[0m_quad_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 547 \u001b[0;31m            \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    548 \u001b[0;31m            \u001b[0mpreds_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    549 \u001b[0;31m            \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> s\n",
      "> \u001b[0;32m/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py\u001b[0m(548)\u001b[0;36m_quad_worker\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    546 \u001b[0;31m        \u001b[0;32mdef\u001b[0m \u001b[0m_quad_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    547 \u001b[0;31m            \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 548 \u001b[0;31m            \u001b[0mpreds_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    549 \u001b[0;31m            \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    550 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb> s\n",
      "> \u001b[0;32m/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py\u001b[0m(549)\u001b[0;36m_quad_worker\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    547 \u001b[0;31m            \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    548 \u001b[0;31m            \u001b[0mpreds_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 549 \u001b[0;31m            \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    550 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    551 \u001b[0;31m            \u001b[0;32mfor\u001b[0m \u001b[0mc_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_last\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_in_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py\u001b[0m(551)\u001b[0;36m_quad_worker\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    549 \u001b[0;31m            \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    550 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 551 \u001b[0;31m            \u001b[0;32mfor\u001b[0m \u001b[0mc_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_last\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_in_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    552 \u001b[0;31m                \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    553 \u001b[0;31m                \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquad_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> C\n",
      "array([  1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15,   1.97831831e+15,   1.97831831e+15,\n",
      "         1.97831831e+15], dtype=float32)\n",
      "ipdb> h_i\n",
      "array([[  7.13341343e-16,   7.04132930e-16,   7.11919598e-16, ...,\n",
      "          7.10254067e-16,   7.03531484e-16,   7.04990339e-16],\n",
      "       [  7.07047941e-16,   6.97918091e-16,   7.05635990e-16, ...,\n",
      "          7.03985176e-16,   6.97321991e-16,   6.98767930e-16],\n",
      "       [  7.00748610e-16,   6.91700128e-16,   6.99349259e-16, ...,\n",
      "          6.97713109e-16,   6.91109322e-16,   6.92542343e-16],\n",
      "       ..., \n",
      "       [  2.99174022e-16,   2.95281615e-16,   2.98575461e-16, ...,\n",
      "          2.97857627e-16,   2.95035022e-16,   2.95636654e-16],\n",
      "       [  2.96489325e-16,   2.92631884e-16,   2.95896137e-16, ...,\n",
      "          2.95185873e-16,   2.92388600e-16,   2.92983720e-16],\n",
      "       [  2.93863496e-16,   2.90040201e-16,   2.93275549e-16, ...,\n",
      "          2.92570500e-16,   2.89798003e-16,   2.90387802e-16]], dtype=float32)\n",
      "ipdb> bt\n",
      "  \u001b[0;32m/home/utkarshu/miniconda3/lib/python3.6/bdb.py\u001b[0m(465)\u001b[0;36mruncall\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m    463 \u001b[0m        \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    464 \u001b[0m        \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 465 \u001b[0;31m            \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    466 \u001b[0m        \u001b[0;32mexcept\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    467 \u001b[0m            \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "  \u001b[0;32m/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py\u001b[0m(576)\u001b[0;36mpredict_train\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m    572 \u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mpredict_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_threaded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    573 \u001b[0m        \u001b[0;34m\"\"\"Make (time, event) predictions on the training data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    574 \u001b[0m        return self.predict(event_in_seq=data['train_event_in_seq'],\n",
      "\u001b[1;32m    575 \u001b[0m                            \u001b[0mtime_in_seq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_time_in_seq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 576 \u001b[0;31m                            single_threaded=single_threaded)\n",
      "\u001b[0m\n",
      "  \u001b[0;32m/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py\u001b[0m(559)\u001b[0;36mpredict\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m    557 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    558 \u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0msingle_threaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 559 \u001b[0;31m            \u001b[0mall_time_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_quad_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    560 \u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    561 \u001b[0m            \u001b[0;32mwith\u001b[0m \u001b[0mMP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "  \u001b[0;32m/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py\u001b[0m(559)\u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m    557 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    558 \u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0msingle_threaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 559 \u001b[0;31m            \u001b[0mall_time_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_quad_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    560 \u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    561 \u001b[0m            \u001b[0;32mwith\u001b[0m \u001b[0mMP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "> \u001b[0;32m/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py\u001b[0m(551)\u001b[0;36m_quad_worker\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    549 \u001b[0;31m            \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    550 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 551 \u001b[0;31m            \u001b[0;32mfor\u001b[0m \u001b[0mc_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_last\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_in_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    552 \u001b[0;31m                \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    553 \u001b[0;31m                \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquad_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb> print(bt)\n",
      "[[ 35.22102356]]\n",
      "ipdb> n\n",
      "> \u001b[0;32m/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py\u001b[0m(552)\u001b[0;36m_quad_worker\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    550 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    551 \u001b[0;31m            \u001b[0;32mfor\u001b[0m \u001b[0mc_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_last\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_in_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 552 \u001b[0;31m                \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    553 \u001b[0;31m                \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquad_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    554 \u001b[0;31m                \u001b[0mpreds_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_last\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py\u001b[0m(553)\u001b[0;36m_quad_worker\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    551 \u001b[0;31m            \u001b[0;32mfor\u001b[0m \u001b[0mc_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_last\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_in_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    552 \u001b[0;31m                \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 553 \u001b[0;31m                \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquad_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    554 \u001b[0;31m                \u001b[0mpreds_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_last\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    555 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> args\n",
      "params = (0, array([[  7.13341343e-16,   7.04132930e-16,   7.11919598e-16, ...,\n",
      "          7.10254067e-16,   7.03531484e-16,   7.04990339e-16],\n",
      "       [  7.07047941e-16,   6.97918091e-16,   7.05635990e-16, ...,\n",
      "          7.03985176e-16,   6.97321991e-16,   6.98767930e-16],\n",
      "       [  7.00748610e-16,   6.91700128e-16,   6.99349259e-16, ...,\n",
      "          6.97713109e-16,   6.91109322e-16,   6.92542343e-16],\n",
      "       ..., \n",
      "       [  2.99174022e-16,   2.95281615e-16,   2.98575461e-16, ...,\n",
      "          2.97857627e-16,   2.95035022e-16,   2.95636654e-16],\n",
      "       [  2.96489325e-16,   2.92631884e-16,   2.95896137e-16, ...,\n",
      "          2.95185873e-16,   2.92388600e-16,   2.92983720e-16],\n",
      "       [  2.93863496e-16,   2.90040201e-16,   2.93275549e-16, ...,\n",
      "          2.92570500e-16,   2.89798003e-16,   2.90387802e-16]], dtype=float32))\n",
      "ipdb> print(args)\n",
      "(1.9783183e+15, array([[  7.89925014e-09]], dtype=float32))\n",
      "ipdb> quad_func(0, c_, wt)\n",
      "array([[ 0.]], dtype=float32)\n",
      "ipdb> quad_func(np.inf, c_, wt)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:40: RuntimeWarning: invalid value encountered in multiply\n",
      "  return c * t * np.exp(-w * t - (c / w) * (np.exp(-w * t) - 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ nan]], dtype=float32)\n",
      "ipdb> quad_func(100, c_, wt)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utkarshu/prog/work/tf_rmtpp/src/tf_rmtpp/rmtpp_core.py:40: RuntimeWarning: overflow encountered in exp\n",
      "  return c * t * np.exp(-w * t - (c / w) * (np.exp(-w * t) - 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ inf]], dtype=float32)\n",
      "ipdb> np.exp(-wt * 100)\n",
      "array([[ 0.99999923]], dtype=float32)\n",
      "ipdb> -wt * t - (c_ / wt) * (np.exp(-wt * t) - 1)\n",
      "*** NameError: name 't' is not defined\n",
      "ipdb> -wt * 100 - (c_ / wt) * (np.exp(-wt * 100) - 1)\n",
      "array([[  1.94058992e+17]], dtype=float32)\n",
      "ipdb> quit\n"
     ]
    }
   ],
   "source": [
    "pdb.runcall(rmtpp_mdl.predict_train, data=data, single_threaded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1990, 100, 1999)"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1999)"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['test_time_out_seq'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1990"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(time_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1999"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(time_preds[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-527-e71bfa62a78a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtime_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "time_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.67356926]], dtype=float32)"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmtpp_mdl.sess.run(rmtpp_mdl.wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1363.0"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.asarray(time_preds).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(time_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "range parameter must be finite.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-511-67d498c8a1e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/utkarshu/miniconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mhist\u001b[0;34m(x, bins, range, normed, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   3079\u001b[0m                       \u001b[0mhisttype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhisttype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3080\u001b[0m                       \u001b[0mrwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3081\u001b[0;31m                       stacked=stacked, data=data, **kwargs)\n\u001b[0m\u001b[1;32m   3082\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3083\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/utkarshu/miniconda3/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1896\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1897\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/utkarshu/miniconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mhist\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   6193\u001b[0m             \u001b[0;31m# this will automatically overwrite bins,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6194\u001b[0m             \u001b[0;31m# so that each histogram uses the same bins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6195\u001b[0;31m             \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhist_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6196\u001b[0m             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# causes problems later if it's an int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6197\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmlast\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/utkarshu/miniconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mhistogram\u001b[0;34m(a, bins, range, normed, weights, density)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         raise ValueError(\n\u001b[0;32m--> 670\u001b[0;31m             'range parameter must be finite.')\n\u001b[0m\u001b[1;32m    671\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mmn\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: range parameter must be finite."
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAFOCAYAAACmIjX8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niRNrAEicWANA4v4LBRfhNhvCKosAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdd904d9588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(time_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring from ./save.rmtpp/model.ckpt-799\n",
      "INFO:tensorflow:Restoring parameters from ./save.rmtpp/model.ckpt-799\n",
      "Starting epoch... 1\n",
      "Loss during batch 0 last BPTT = 3892142519466590208.000, lr = 0.08622\n",
      "Loss on last epoch = 3892142519466590208.0000, new lr = 0.08622, global_step = 1598\n",
      "Starting epoch... 2\n",
      "Loss during batch 0 last BPTT = 4477427402338729984.000, lr = 0.08066\n",
      "Loss on last epoch = 4477427402338729984.0000, new lr = 0.08066, global_step = 2397\n",
      "Starting epoch... 3\n",
      "Loss during batch 0 last BPTT = 3853046085006131200.000, lr = 0.07578\n",
      "Loss on last epoch = 3853046085006131200.0000, new lr = 0.07578, global_step = 3196\n",
      "Starting epoch... 4\n",
      "Loss during batch 0 last BPTT = 3826016240904699904.000, lr = 0.07145\n",
      "Loss on last epoch = 3826016240904699904.0000, new lr = 0.07145, global_step = 3995\n",
      "Starting epoch... 5\n",
      "Loss during batch 0 last BPTT = 3895325055873187840.000, lr = 0.06759\n",
      "Loss on last epoch = 3895325055873187840.0000, new lr = 0.06759, global_step = 4794\n",
      "Starting epoch... 6\n",
      "Loss during batch 0 last BPTT = 3255386198763372544.000, lr = 0.06413\n",
      "Loss on last epoch = 3255386198763372544.0000, new lr = 0.06413, global_step = 5593\n",
      "Starting epoch... 7\n",
      "Loss during batch 0 last BPTT = 3218644918209609728.000, lr = 0.06101\n",
      "Loss on last epoch = 3218644918209609728.0000, new lr = 0.06101, global_step = 6392\n",
      "Starting epoch... 8\n",
      "Loss during batch 0 last BPTT = 3438362525811343360.000, lr = 0.05817\n",
      "Loss on last epoch = 3438362525811343360.0000, new lr = 0.05817, global_step = 7191\n",
      "Starting epoch... 9\n",
      "Loss during batch 0 last BPTT = 5279453514836213760.000, lr = 0.05559\n",
      "Loss on last epoch = 5279453514836213760.0000, new lr = 0.05559, global_step = 7990\n",
      "Model saved at ./save.rmtpp/model.ckpt\n",
      "CPU times: user 11min 31s, sys: 0 ns, total: 11min 31s\n",
      "Wall time: 4min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rmtpp_mdl.train(training_data=data, restart=True, with_summaries=True, num_epochs=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5625"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['train_event_in_seq']) / rmtpp_mdl.BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799.9"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(data['train_event_in_seq'][0]) / rmtpp_mdl.BPTT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7999"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(data['train_event_in_seq'][0])) #  / rmtpp_mdl.BATCH_SIZE) / rmtpp_mdl.BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00921901,  0.00011524])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train_time_in_seq'][[2, 3], :][:, 4] - data['train_time_in_seq'][[2, 3], :][:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05606906,  0.05609789,  0.12283624, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.03780895,  0.05659572,  0.0711524 , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train_time_in_seq'][[0, 1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05609789,  0.12283624,  0.12970529, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.05659572,  0.0711524 ,  0.09084344, ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train_time_out_seq'][[0, 1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "719"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['train_time_in_seq'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99998044189529367"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(data['train_time_in_seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99920974436497556"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(data['test_time_in_seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1326"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['test_event_in_seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmtpp_mdl.BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(730, 1326)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(h_preds), len(events_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0. ,  0.2,  0. ,  0. ,  0.2,  0.2,  0.2,  0. ,  0. ,  0.2,  0. ,\n",
       "        0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "        0. ], dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_preds[29][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0. ,  0.2,  0. ,  0. ,  0.2,  0.2,  0.2,  0. ,  0. ,  0.2,  0. ,\n",
       "        0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "        0. ], dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_preds[0][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.273139e-06"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterSession.run(rmtpp_mdl.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def foo(n, p):\n",
    "    return np.exp((1 - p + np.log(p)) / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utkarshu/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fe21bbee780>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAFbCAYAAAA9VUwCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhRBCCAOTQi+EEEIYmBR6IYQQwsD+DzzcOcP4vY/7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe21bc6fb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = np.arange(0, 1, 0.01)\n",
    "plt.plot(p, foo(0.01, p), label='0.01')\n",
    "plt.plot(p, foo(0.1, p), label='0.1')\n",
    "plt.plot(p, foo(0.5, p), label='0.5')\n",
    "plt.plot(p, foo(1.0, p), label='1.0')\n",
    "# plt.plot(p, foo(2.0, p), label='2.0')\n",
    "plt.plot(p, p, label='x = y')\n",
    "plt.ylabel(r'$P[t_{next} > m^{-1}(\\theta)]$')\n",
    "plt.xlabel(r'Threshold ($\\theta$)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Soft-min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def soft_min(x):\n",
    "    \"\"\"Provides a soft version of the function min(1, x).\"\"\"\n",
    "    return (np.exp(2.1 * x) - 1)/(np.exp(2.1 * x) + 1)\n",
    "\n",
    "def hard_min(x):\n",
    "    return np.minimum(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdc20124518>]"
      ]
     },
     "execution_count": 884,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAFKCAYAAAAuZDceAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMdn/ByI0LrajD7FRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdc20124550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0, 5, 0.01)\n",
    "y_hard = hard_min(x)\n",
    "y_soft = soft_min(x)\n",
    "plt.plot(x, y_hard, 'k')\n",
    "plt.plot(x, y_soft, '--b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ,\n",
       "        1. ,  1. ,  1. ,  1. ,  1. ,  1. ,  1. ,  1. ,  1. ])"
      ]
     },
     "execution_count": 868,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hard_min(np.arange(0, 2, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.10461582,  0.2069665 ,  0.30497892,  0.39693043,\n",
       "        0.4815498 ,  0.55805222,  0.62611477,  0.68580906,  0.73751106,\n",
       "        0.78180636,  0.81940371,  0.85106411,  0.87754767,  0.89957745,\n",
       "        0.91781744,  0.93286155,  0.94523038,  0.95537312,  0.96367262])"
      ]
     },
     "execution_count": 885,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_min(np.arange(0, 2, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
